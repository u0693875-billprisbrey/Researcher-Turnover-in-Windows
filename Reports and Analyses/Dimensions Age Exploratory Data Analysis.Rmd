---
title: "Dimensions Age Exploratory Data Analysis"
author: "Bill Prisbrey"
date: "2025-06-10"
output:
  html_document:
    keep_md: true
---

To Do:
Limit the query to a date
Remove the "As of today" phrase
Adjust plot sizes
A few more words and it's done
Refer to "target population"

```{r settings, include = FALSE}

knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message=FALSE, error=FALSE,
                      fig.height=5, fig.width=7)

# fig.height=6.75, fig.width=12) for ppt
# fig.height=7, fig.width=10) typical report


# adjust graphic parameters

oldPar <- par(cex.main = 3,
    cex.lab = 3,
    cex.axis = 2,
    mar = c(5.1,4.1,4.1,2.1) # default is c(5.1,4.1,4.1,2.1)
    )

# par(oldPar) #restore old parameters # restore old parameters after the plot

```

```{r}

# developing a function to display categorical differences

plotCategory <- function(colname, wanted_label=NA) {
  
  table(fullPI[,c(colname, "class")]) |> 
  (\(x){ 
    
if (!all(is.na(wanted_label))) {
        original_names <- rownames(x)
        new_names <- ifelse(original_names %in% wanted_label, original_names, "")
        dimnames(x)[[1]] <- new_names
      }
    
    plot(x,
         main = gsub("_", " ", gsub("PROPOSAL_","", colname)),
         col = c("chocolate4","thistle"),
         cex = 1.2)
        
    })()
  
}


```



```{r}

##########
## LOAD ##
##########

# PREP SCRIPT

source(here::here("Prep scripts","Adjusting prepData and loading things.R"))

# LOAD MODEL 

xgbModel <- readRDS(here::here("Robjects", "Identifying missing age values from Dimensions Age Exploratory Data Analysis.rds"))

# PER PI

piClusters <- lapply(list.files(here::here("Robjects", "Clustering PIs"), full.names = TRUE), readRDS)

###########
## MERGE ## 
###########

piEmplid <- Reduce(function(x, y) {
  merged <- merge(x, y, by = "emplid", all = FALSE)
  merged <- merged[, !duplicated(sub("\\.x$|\\.y$", "", names(merged)))]  # Remove duplicate columns
  
  # Rename columns to remove ".x"
  names(merged) <- sub("\\.x$", "", names(merged))
  
  merged
}, piClusters)
# piEmplid <- Reduce(function(x, y) merge(x, y, by = "emplid", all = FALSE), piClusters)
# piEmplid <- do.call(merge, piClusters)



prepData <- merge(prepData, piEmplid[,c("emplid","complex_cluster","rate_cluster")], by.x =  "PROPOSAL_PI_EMPLID", by.y = "emplid", all.x = TRUE)

##########
## PREP ##
##########

piEmplid$combined_cluster <-  factor(paste(piEmplid$complex_cluster, piEmplid$rate_cluster, sep = ", "))

prepData$combined_cluster <- factor(paste(prepData$complex_cluster, prepData$rate_cluster, sep = ", "))

###############
## LIBRARIES ##
###############

library(viridis)
library(pheatmap)

#########
## MAP ##
#########

piMap <- data.frame(college = row.names(piEmplid), abbrv = row.names(piEmplid), color = NA, pch = 19, cex = 0.7 )

# I need the college map of colors

fullEmplid <- calculateWinRates(data = cleanData, categoryColumn = "PROPOSAL_PI_EMPLID") |>
  (\(x){x[[1]]})()

fullEmplid$count.total <- apply(fullEmplid[,c("win.count","loss.count")],1,sum)

filterTwoCount <- fullEmplid$count.total >= 2
# filterThreeCount <- piEmplid$count.total > 3


```


```{r}
             
# Manage colors

#############
## COMPLEX ##
#############

complexClusterColors <- c("forestgreen", "deepskyblue", "goldenrod",  
                            "firebrick", "darkslategray", "chartreuse",  
                            "slateblue", "darkkhaki", "coral")

names(complexClusterColors) <- c(as.character(1:5),"unassigned") # I'll surely regret this

clusterComplexMapping <- piMap
clusterComplexMapping[,"pch"] <- rep(19, nrow(clusterComplexMapping))

clusterComplexMapping[,"color"] <- complexClusterColors [piEmplid$complex_cluster]

# clusterComplexMapping[,"color"] <- complexClusterColors[complexHCPC$data.clust$clust[match(clusterComplexMapping[,"college"], row.names(complexHCPC$data.clust))] ]

##############
## COMBINED ##
##############

combinedClusterColors <- c("forestgreen", "deepskyblue", "goldenrod",  
  "firebrick", "darkslategray", "chartreuse",  
  "slateblue", "darkkhaki", "coral",  
  "mediumorchid", "dodgerblue", "tomato",  
  "orchid", "darkseagreen", "sienna",  
  "royalblue", "indianred", "seagreen",  
  "peru", "cadetblue", "plum",  
  "midnightblue", "lawngreen", "darkorange",  
  "lightsteelblue")

clusterCombinedMapping <- piMap
clusterCombinedMapping[,"pch"] <- rep(19, nrow(clusterCombinedMapping))

clusterCombinedMapping[,"color"] <- combinedClusterColors [piEmplid$combined_cluster]



################
## PERCENTILE ##
################

percentileColors <- viridis(10,direction = -1)[c(1:7,10)]
percentileMapping <- piMap

percentileMapping[,"pch"] <- rep(19, nrow(percentileMapping))

percentileMapping[,"color"] <- percentileColors[as.numeric(cut(piEmplid$win.sum_percentile, breaks = seq(0.2,1,by=0.1)))]

#############
## COLLEGE ##
#############

collegeAbbrv <- cbind(
  college = c(sweet16, bigInst, "other"),
  abbrv = c("Arch",
            "Educ",
            "FinArt",
            "Health",
            "Hum",
            "Nurs",
            "Pharm",
            "Science",
            "SocBeh",
            "SocWrk",
            "Bus",
            "Law",
            "Tran",
            "Dent",
            "Med",
            "Engr",
            "EGI",
            "Hunt",
            "SCI",
            "CVRTI",
            "ICSE",
            "CTSI",
            "other"
  ),
  color = c(
    "lightslategray",
    "orange",
    "cyan", 
    "hotpink", 
    "brown", 
    "darkgoldenrod", 
    "gold", 
    "green",
    "navy", 
    "magenta", 
    "olivedrab4", 
    "salmon", 
    "darkgreen",
    "yellowgreen", 
    "red",
    "blue",
    "chocolate", 
    "purple",
    "violet", 
    "khaki",
    "deepskyblue3",
    "chartreuse",
    "darkmagenta"
  ),
  pch = c(
    1,14,15,2,3,4,17,6,5,8,9,10,11,12,13,0,16,7,18, 23, 24, 25, 20  
    
    
  ),
  
  cex = rep(NA, length(c(sweet16, bigInst, "other")) )
)

collegeColors <- setNames(collegeAbbrv[,"color"], collegeAbbrv[,"abbrv"])

```


```{r}

###########
## QUERY ##
###########

# Obtain age data

keyring::keyring_unlock(keyring = "BIPR", password = "Excelsior!")

library(DBI)
con.ds <- DBI::dbConnect(odbc::odbc(), Driver = "oracle", Host = "ocm-campus01.it.utah.edu", 
                         SVC = keyring::key_list(keyring = "BIPR")[1, 1], UID = keyring::key_list(keyring = "BIPR")[1, 
                                                                                                                    2], PWD = keyring::key_get(keyring = "BIPR", service = keyring::key_list(keyring = "BIPR")[1, 
                                                                                                                                                                                                               1]), Port = 2080)

ageQuery <- "
select
               dc.emplid,
               daa.first_pub_yr,
               daa.last_pub_yr,
               daa.total_publications,
               daa.first_pub_title,
               daa.first_pub_id,
               daa.first_grant_yr,
               daa.last_grant_yr,
               daa.total_grants,
               dc.dim_id,
               dc.confidence,
               dc.confidence_desc
from vpr_dimensions.dimensions_crosswalk dc
left join vpr_dimensions.ds_uu_academic_age daa on daa.uu_researcher_dim_id = dc.dim_id
"

ageData <- dbGetQuery(con.ds,
                      ageQuery)

DBI::dbDisconnect(con.ds)


```


```{r}

##########
## PREP ##
##########

ageData$CONFIDENCE <- factor(ageData$CONFIDENCE, levels = c(1:5, "W"))

###########
## MERGE ##
###########

prepAgeData <- merge(prepData, ageData, by.x = "PROPOSAL_PI_EMPLID" , by.y = "EMPLID", all.x = TRUE)

# due to ambiguous matches, there are multiple matches for some EMPLIDs

# length(unique(prepAgeData$PROPOSAL_PI_EMPLID[is.na(prepAgeData$CONFIDENCE)])) # 812 !

prepAgeData$class <- ifelse(is.na(prepAgeData$CONFIDENCE),"missing","present")

```



```{r}

## IDENTIFY DUPLICATE MATCHES

theDupes <- table(ageData[,c("EMPLID", "DIM_ID")]) |>
  as.data.frame.matrix() |>
    (\(x){apply(x, 1, sum)  })() |>
    (\(x){x[x>1]})()

nonDupes <- unique(ageData$EMPLID)[!unique(ageData$EMPLID) %in% names(theDupes)]  

# DESCRIBE TARGET POPULATION

missingSubset <- unique(prepAgeData$PROPOSAL_PI_EMPLID[is.na(prepAgeData$CONFIDENCE)])

matchedSubset <- unique(prepAgeData$PROPOSAL_PI_EMPLID[!is.na(prepAgeData$CONFIDENCE)])

singleSubset <- droplevels(unique(prepAgeData$PROPOSAL_PI_EMPLID[prepAgeData$PROPOSAL_PI_EMPLID %in% nonDupes]))

multipleSubset <- droplevels(unique(prepAgeData$PROPOSAL_PI_EMPLID[prepAgeData$PROPOSAL_PI_EMPLID %in% names(theDupes)]))

missingPercentage <- paste0(
  round(
    proportions(
      c(length(missingSubset), length(multipleSubset), length(singleSubset)
        )  
      )
    [1]*100,0),"%")

multiplePercentage <- paste0(
  round(
    proportions(
      c(length(missingSubset), length(multipleSubset), length(singleSubset)
        )  
      )
    [2]*100,0),"%")

singlePercentage <- paste0(
  round(
    proportions(
      c(length(missingSubset), length(multipleSubset), length(singleSubset)
        )  
      )
    [3]*100,0),"%")


```



**PURPOSE:**  The purpose of this report is to describe aligning the "Dimensions" data set with the target population of PI's and draw conclusions on its utility. 

**OBJECTIVES:**   

  1.  Describe the Dimensions data.   
  2.  Describe alignment with the target population.    
  3.  Identify patterns in missing matches.

**EXECUTIVE SUMMARY:**

The Dimensions data set is an external data set that tracks activity by researchers.  It could be used to identify the first publication (grant?) by a researcher, and use this date as a proxy for the birthdate, deriving an "academic age" per PI.

This Dimensions data set is manually curated and aligned with internal PI's using clues like name, department, and title.  However, the alignment is time-consuming and imperfect.

As of today, `r today()`, `r length(unique(ageData$EMPLID)) |> format(big.mark = ",")` PI's have potential matches with `r length(unique(ageData$DIM_ID)) |> format(big.mark = ",")` researchers identified in Dimensions with varying counts of matches and varying levels of confidence.  

However, of the target population of `r length(unique(prepData$PROPOSAL_PI_EMPLID)) |> format(big.mark = ",") ` PI's, some `r missingPercentage` have no matches.   Further complicating an analysis, these missing matches are not random.  A predictive machine learning model scored a Kappa value of 0.29 where a Kappa value of less than 0.1 would indicate randomness.  Instead, the missing PI's tend to be older, with a sparser activity record outside of the main researching institutions at the U. 

As well, there are internal inconsistencies in the confidence rating, where PI's have multiple potential matches but are scored as having a one-to-one match.

In conclusion, the data set is unattractive to use in further analysis, specifically in determining an "academic age" per PI, because it is too incomplete, too biased in missing values, and too inconsistent.  Alternative data should be explored before using this data.   

  
**SUMMARY:**   

The Dimensions data set is an external data set that tracks activity by researchers.  It is described [here.] (Find a link.)  

Internally, the Office of Sponsored Projects similarly tracks proposal activity by principal investigators.

The internal and external data sets can be aligned in order to have a more comprehensive evaluation of an individual's activity, both preceding and succeeding their time at the U.

For the purposes of evaluating PI turnover and retention, it is hoped that an "academic age" per PI can be derived from this data set based on their earliest publication [or grant?].  The "academic age" will be used as a proxy for actual age, as the employees' birthdates is unavailable for analyses because this information is considered sensitive and subject to maximum restriction.

The internal and external data sets are aligned by matching researcher identification through clues such as name, department, and publication title.  However, external data sets are created through a mish mash of methods and adhere to varying rules and quality standards. A perfect alignment with internal data, which verifiably follows a number of rules and quality standards, is not expected.

For the purposes of this analysis, date fields describing first and last publishing year or grant year, total number of publications and grants, and an alignment confidence score are extracted from the external data set.

The publication dates range from the year `r min(ageData$FIRST_PUB_YR, na.rm = TRUE)` to `r max(ageData$LAST_PUB_YR, na.rm = TRUE)`, and the grant dates range from the year `r min(ageData$FIRST_GRANT_YR, na.rm = TRUE)` to `r max(ageData$LAST_GRANT_YR, na.rm = TRUE)`.  `r sum(ageData$TOTAL_PUBLICATIONS, na.rm = TRUE) |> format(big.mark = ',')` publications and `r sum(ageData$TOTAL_GRANTS, na.rm = TRUE) |> format(big.mark = ',')` grants are tallied.

The combined data set attempts to align `r length(unique(ageData$EMPLID))` internal employee identification numbers with `r length(unique(ageData$DIM_ID))` external researcher identification numbers and provides a score of confidence in the alignment.  It aligns `r length(unique(ageData$EMPLID[ageData$EMPLID %in% nonDupes & ageData$CONFIDENCE == 1])) |> format(big.mark = ",")` (`r  (length(unique(ageData$EMPLID[ageData$EMPLID %in% nonDupes & ageData$CONFIDENCE == 1])) / length(unique(ageData$EMPLID))) |> (\(x){round(x*100,0)})() `%) internal EMPLIDs with a one-to-one match at the highest confidence score.  The remaining `r length(unique(ageData$EMPLID[!(ageData$EMPLID %in% nonDupes) | ageData$CONFIDENCE != 1]))` EMPLID's are aligned with multiple potential external ID's at reduced confidence levels.  

(Oddly, `r length(unique(ageData$EMPLID[ageData$EMPLID %in% names(theDupes) & ageData$CONFIDENCE == 1]))` EMPLID's are aligned with multiple external ID's with a confidence level of 1.)

However, internally OSP has tracked the research activity 



This evaluation found that 





Wow I'm struggling to write anything coherent.

The intent is to identify the first publication date per PI.

I actually don't need many words on this one.  The point is to get it done.  It's just a quick look at what we have.

Where's my graphic of how many are matched and un-matched?

It's like, instead of three tiles describing "duplicate" I need to add a fourth one on the right, the first one, which is

... is ....

... hold on ...

..."subset of interest" that is "found" and "not found" ?

I guess it's just another row of three.

Let's do that, first, and see what it looks like.




I'm getting tired of this.

I need to simply conclude that this external data set is not usable for the purposes of this study due to a large proportion of non-random missing data in the target population.  As well, concerns about consistency of definitions are noted (where PI's with multiple matches are scored with a self-contradictory 'one-to-one' match rating.)

## (1) DESCRIBE THE DIMENSIONS DATA

### Data summary

```{r}

skim(ageData)

```



### Grants and publication dates

```{r}

# First and last publication dates

plotPar <- par(mfrow = c(4,1), 
               bg = "ivory", 
               fg = "gray20",
               oma = c(1,0,0,0),
               mar = c(3, 4.1, 2, 0.3)
               )

#mar = c(2, 4.1, 1.1, 0.1)) # c(5.1,4.1,4.1,2.1)

table(ageData$FIRST_PUB_YR) |>
  plot(ylab = "Count of PI's",
       main = "Initial publication year",
       type = "l",
       las = 1,
       col = "forestgreen")


table(ageData$LAST_PUB_YR) |> log () |>
  plot(ylab = "Log count of PI's",
       main = "Final publication year",
       type = "b",
       cex = 0.619,
       las = 1,
       col = "darkolivegreen3")



table(ageData$FIRST_GRANT_YR) |>
  plot(ylab = "Count of PI's",
       main = "Initial grant year",
       type = "l",
       las = 1,
       col = "purple4")


table(ageData$LAST_GRANT_YR) |> log () |>
  plot(ylab = "Log count of PI's",
       main = "Final grant year",
       type = "b",
       cex = 0.619,
       las = 1,
       col = "orchid")


par(plotPar)


```

### Grants and publications

```{r}

plotPar <- par(mfrow = c(2,1), 
               bg = "ivory", 
               fg = "gray20",
               oma = c(1,0,0,0),
               mar = c(3, 4.1, 2, 0.3)
               )



hist(ageData$TOTAL_PUBLICATIONS, col = "darkolivegreen3")

hist(ageData$TOTAL_GRANTS, col = "orchid")

par(plotPar)

```

### Matching confidence explained


```{r}

unique(ageData[, c("CONFIDENCE", "CONFIDENCE_DESC")]) |>
  (\(x){
    x[order(match(x$CONFIDENCE, levels(ageData$CONFIDENCE))),]
    
  })() |>
  kbl(caption = "Confidence ratings and description", row.names = FALSE, col.names = c("", "DESCRIPTION")  ) |>
  column_spec(2, extra_css = "padding-left: 1em;")


```

### Matches and confidence scores of the available data

```{r fig.height = 6, fig.width= 9}

# Combining graphic of dupes and non-dupes

# parameters
# incomingPar <- par(mfrow = c(1,3),
#              mar = c(1,2,0,1),
#              oma = c(0,0,2,0))

# Create the layout 
layout(matrix(c(1,1,1,1,2,3,4,5), nrow = 2, byrow = TRUE),
       heights = c(0.3, 1)) 

incomingPar <- par(mar = c(4,3,4,2),
                   oma = c(0,0,3,1))

# Dupes and non-dupes barplot

firstPlot <- barplot(as.matrix(c(length(theDupes), length(unique(ageData$EMPLID)[ageData$EMPLID %in% nonDupes]  ))),
        col = c("pink","gray"),
        main = "COUNT OF PI'S", main.lab = 2,
        horiz = TRUE
        )

singlePercentage <- paste(round(proportions(c(length(theDupes), length(unique(ageData$EMPLID)[ageData$EMPLID %in% nonDupes]  )))[2]*100,0),"%")

text(x = 4000, y = firstPlot,  paste0("SINGLE (", singlePercentage, ")") , col = "purple", cex = 1.6, font =2 )

# text(x = 4000, y = firstPlot,  "SINGLE", col = "purple", cex = 1.6, font =2 )

# text(x = 4000, y = firstPlot*0.7,  paste(round(proportions(c(length(theDupes), length(unique(ageData$EMPLID)[ageData$EMPLID %in% nonDupes]  )))[2]*100,0),"%"), col = "purple", cex = 1.6, font =2 )


text(x = 0.619*length(theDupes), y = firstPlot,  "MULTIPLE", col = "gray30", cex = 1.6, font = 2)


# Graphing duplicates first

par(mar = c(4,4,4,2))

boxplot(theDupes, col = "pink"
        )

mtext(side = 2,
      text = "MATCHES per PI",
      line = 2.5,
      font =2,
      col = "gray20",
      cex = 0.7)


table(ageData$CONFIDENCE[ageData$EMPLID %in% names(theDupes)]) |>
  barplot(col = c(viridis::viridis(5)[1:5],"grey"),
    ylab = "COUNT of MATCHES",
    font.lab = 2,
    col.lab = "gray20",
    xlab = "CONFIDENCE",)

mtext("MULTIPLE PI MATCHES", at = -2, line = 2, xpd = TRUE)

# mtext("PI duplication and confidence score for duplicate PI matches", outer = TRUE)



# Graphing non-dupes

secondPlot <- barplot(as.matrix(table(ageData$CONFIDENCE[ageData$EMPLID %in% nonDupes])), col = c(viridis::viridis(5),"gray"),
                      border = NA,
                      ylab = "COUNT of PI'S", font.lab = 2,
                      col.lab = "purple")

text(x=secondPlot, y = table(ageData$CONFIDENCE[ageData$EMPLID %in% nonDupes])[1]*0.619, "CONFIDENCE = 1", col = "ivory", font = 2 , cex = 1.3)

text(x=secondPlot, y = table(ageData$CONFIDENCE[ageData$EMPLID %in% nonDupes])[1]*0.619 * 0.85, paste0(round(proportions(table(ageData$CONFIDENCE[ageData$EMPLID %in% nonDupes]))[1]*100,0),"%") , col = "ivory", font = 2 , cex = 1.3)


thirdPlot <- ageData$CONFIDENCE[ageData$EMPLID %in% nonDupes] |>
  table() |>
  (\(x){
    x[names(x) != "1"]
    
  })() |>
  barplot(
    ylab = "COUNT of PI'S",
    font.lab = 2,
    col.lab = "purple",
    xlab = "CONFIDENCE",
    #ylab = expression(CONFIDENCE != 1),
    #font.lab = 2,
    #col.lab = "gray20",
    col = c(viridis::viridis(5)[2:5],"grey") )


mtext("SINGLE PI MATCHES", at = -2, line = 2, xpd = TRUE)

# mtext("PI duplication and confidence score for non-duplicated PI matches", outer = TRUE)

mtext("Matches per PI and confidence scores of available data", outer = TRUE, font = 2, cex = 1.381)

par(incomingPar)

# It would be great if I had a floating title over the two sub-graphics # DONE!

```


```{r eval=FALSE}

# I combined the two graphics so I don't need this anymore

# Graphing duplicates

incomingPar <- par(mfrow = c(1,3),
              mar = c(2,4,0,0),
              oma = c(0,0,2,0))

firstPlot <- barplot(as.matrix(c(length(theDupes), length(unique(ageData$EMPLID)[ageData$EMPLID %in% nonDupes]  ))),
        col = c("pink","gray"),
         ylab = "COUNT OF PI'S", font.lab = 2
        )

text( x = firstPlot, y = 4000, "NON-DUPLICATED", col = "purple", cex = 1.6, font =2 )

text( x = firstPlot, y = 4000*0.9, paste(round(proportions(c(length(theDupes), length(unique(ageData$EMPLID)[ageData$EMPLID %in% nonDupes]  )))[2]*100,0),"%"), col = "purple", cex = 1.6, font =2 )


text(x = firstPlot, y = 1000, "DUPLICATED", col = "gray30", cex = 1.6, font = 2)


boxplot(theDupes, col = "pink"
        )

mtext(side = 2,
      text = "DUPLICATIONS PER PI",
      line = 2.9,
      font =2,
      col = "gray20")


table(ageData$CONFIDENCE[ageData$EMPLID %in% names(theDupes)]) |>
  barplot(col = c(viridis::viridis(5)[1:5],"grey"))

mtext("PI duplication and confidence score for duplicate PI matches", outer = TRUE)

par(incomingPar)

# looks great

# should I drill down?
# nah, that's good enough for this report

```

## (2) DESCRIBE ALIGNMENT WITH THE TARGET POPULATION

Not all `r length(unique(ageData$EMPLID)) |> format(big.mark = ",")` PI's available in the Dimensions data are of immediate interest.  A previous study examined `r length(unique(prepData$PROPOSAL_PI_EMPLID)) |> format(big.mark = ",")` PI's who submitted proposals between FY2013 and FY2023.  This is the target population.   



```{r eval=FALSE}

all(unique(prepData$PROPOSAL_PI_EMPLID) %in% unique(ageData$EMPLID))

unique(prepData$PROPOSAL_PI_EMPLID)[!(unique(prepData$PROPOSAL_PI_EMPLID)) %in% unique(ageData$EMPLID)] |> length() # 812

# Go back to where I calculated all of this and figure out a way to demonstrate it.

# I think the "subset of interest" is reasonable.

# For the subset of interest--
#  - Repeat the pink/gray "duplication" graphics.
#  - State whether the missing values are random.
#  - And I seriously need better words to describe this.

# I think I'll publish what I have and review.

length(unique(prepAgeData$PROPOSAL_PI_EMPLID[is.na(prepAgeData$CONFIDENCE)]))/length(unique(prepAgeData$PROPOSAL_PI_EMPLID))

c(length(unique(prepAgeData$PROPOSAL_PI_EMPLID[is.na(prepAgeData$CONFIDENCE)])),
  length(unique(prepAgeData$PROPOSAL_PI_EMPLID[!is.na(prepAgeData$CONFIDENCE)]))
  ) |>
  as.matrix() |>
  barplot(col = c("chocolate4", "cyan2"),
          main = "Match found and no match found in Dimensions data")

# This is match found and no match found

```  


```{r fig.height = 6, fig.width= 9}

# Subset of interest


# Create the layout 
layout(matrix(c(1,1,1,1,1,2,3,4,5,6), nrow = 2, byrow = TRUE),
       heights = c(0.3, 1)) 

incomingPar <- par(mar = c(4,3,4,2),
                   oma = c(0,0,3,1))

# Missing, dupes, and non-dupes barplot

firstPlot <- barplot(as.matrix(c(length(missingSubset), length(multipleSubset), length(singleSubset))),
        col = c("chocolate4", "pink","gray"),
        main = "COUNT of PI'S IN SUBSET", main.lab = 2,
        horiz = TRUE
        )

text(x = length(missingSubset)*0.5, y = firstPlot,  paste0("MISSING (", missingPercentage, ")") , col = "wheat", cex = 1.3, font =2 )

text(x = (length(missingSubset) + length(multipleSubset))*0.8, y = firstPlot,  paste0("MULTIPLE (", multiplePercentage, ")") , col = "gray30", cex = 1.3, font =2 )

text(x = (length(missingSubset) + length(multipleSubset)+length(singleSubset))*0.9, y = firstPlot,  paste0("SINGLE (", singlePercentage, ")") , col = "purple", cex = 1.3, font =2 )

# text(x = 4000, y = firstPlot,  "SINGLE", col = "purple", cex = 1.6, font =2 )

# text(x = 4000, y = firstPlot*0.7,  paste(round(proportions(c(length(theDupes), length(unique(ageData$EMPLID)[ageData$EMPLID %in% nonDupes]  )))[2]*100,0),"%"), col = "purple", cex = 1.6, font =2 )


# plain plot

plot(1, type = "n", xlab = "", ylab = "", xaxt = "n", yaxt = "n", bty = "n")
rect(xleft = 0.6, ybottom = 0.75, xright = 1.4, ytop = 1.25, col = "chocolate4", border = FALSE)
text(x=1, y= 1, "Missing\nexamined\nbelow", col = "wheat", font = 2, cex = 1.619)

mtext("NO PI MATCHES\n(MISSING)", at = -2, line = 2)

# Graphing duplicates first

par(mar = c(4,4,4,2))

boxplot(theDupes[names(theDupes) %in% multipleSubset], col = "pink"
        )

mtext(side = 2,
      text = "MATCHES per PI",
      line = 2.5,
      font =2,
      col = "gray20",
      cex = 0.7)


table(prepAgeData$CONFIDENCE[prepAgeData$PROPOSAL_PI_EMPLID %in% multipleSubset]) |>
  barplot(col = c(viridis::viridis(5)[1:5],"grey"),
    ylab = "COUNT of MATCHES",
    font.lab = 2,
    col.lab = "gray20",
    xlab = "CONFIDENCE",)

mtext("MULTIPLE PI MATCHES", at = -2, line = 2, xpd = TRUE)

# mtext("PI duplication and confidence score for duplicate PI matches", outer = TRUE)



# Graphing non-dupes

secondPlot <- barplot(as.matrix(table(prepAgeData$CONFIDENCE[prepAgeData$PROPOSAL_PI_EMPLID %in% singleSubset])), col = c(viridis::viridis(5),"gray"),
                      border = NA,
                      ylab = "COUNT of PI'S", font.lab = 2,
                      col.lab = "purple")

text(x=secondPlot, y = table(prepAgeData$CONFIDENCE[prepAgeData$PROPOSAL_PI_EMPLID %in% singleSubset])[1]*0.619, "CONFIDENCE\n= 1", col = "ivory", font = 2 , cex = 1.3)

text(x=secondPlot, y = table(prepAgeData$CONFIDENCE[prepAgeData$PROPOSAL_PI_EMPLID %in% singleSubset])[1]*0.619 * 0.85, paste0(round(proportions(table(prepAgeData$CONFIDENCE[prepAgeData$PROPOSAL_PI_EMPLID %in% singleSubset]))[1]*100,0),"%") , col = "ivory", font = 2 , cex = 1.3)


thirdPlot <- prepAgeData$CONFIDENCE[prepAgeData$PROPOSAL_PI_EMPLID %in% singleSubset] |>
  table() |>
  (\(x){
    x[names(x) != "1"]
    
  })() |>
  barplot(
    ylab = "COUNT of PI'S",
    font.lab = 2,
    col.lab = "purple",
    xlab = "CONFIDENCE",
    #ylab = expression(CONFIDENCE != 1),
    #font.lab = 2,
    #col.lab = "gray20",
    col = c(viridis::viridis(5)[2:5],"grey") )


mtext("SINGLE PI MATCHES", at = -2, line = 2, xpd = TRUE)

# mtext("PI duplication and confidence score for non-duplicated PI matches", outer = TRUE)

mtext("Matches per PI and confidence scores of target population", outer = TRUE, font = 2, cex = 1.381)

par(incomingPar)



```


### (3) IDENTIFY PATTERNS IN MISSING MATCHES



```{r}

####################################################
## USE ML TO SEE IF MISSING VALUES HAVE A PATTERN ##
####################################################

## PREP

### calculate win/loss per pI

fullPI <- calculateWinRates(data = prepData, categoryColumn = "PROPOSAL_PI_EMPLID", functionList = list(mean = mean, median = median)) |> (\(x){ x[[1]] })()

fullPI$count.total <- apply(fullPI[,c("win.count","loss.count")], 1, sum, na.rm = TRUE )
fullPI$sum.total <- apply(fullPI[,c("win.sum","loss.sum")], 1, sum, na.rm = TRUE )

###  extract most common strings and factors for certain columns

theStrings <- c("PROPOSAL_PURPOSE", 
                "PROPOSAL_TYPE", 
                "college",
                "PROPOSAL_RECIPIENT_FUNDING_TYPE",
                "PROPOSAL_FANDA_WAIVER_INDIVATOR",
                "PROPOSAL_COST_SHARE_INDICATOR",
          "PROPOSAL_FANDA_OFF_CAMPUS_RATE_INDICATOR",
                "PROPOSAL_UPLOAD_DATE_FISCAL_YEAR",
                "PROPOSAL_PI_APPOINTMENT_COLLEGE",
                "PROPOSAL_PI_ACADEMIC_RANK",
                "PROPOSAL_PI_TENURE_STATUS",
          "PROPOSAL_PI_FACULTY_CATEGORY",
          "PROPOSAL_PI_FACULTY_SUBCATEGORY",
          "PROPOSAL_PI_ACADEMIC_RANK_LEVEL",
          "PROPOSAL_PI_RANK_SORTED",
          "PROPOSAL_PI_FACULTY_LINE_SORTED",
          "VPR_PROPOSAL_COLLEGE",
          "PROPOSAL_VP",
          "VPR_PROPOSAL_VP",
          "PROPOSAL_SPONSOR_TYPE",
          "PROPOSAL_IACUC_IRB_DIM_KEY",
          "HIGHEST_GOVERNMENT_AGENCY_ACRONYM",
          "NEXT_HIGHEST_GOVERNMENT_AGENCY_ACRONYM",
          "complex_cluster",
          "rate_cluster"
                )

mostCommon_list <- lapply(theStrings, function(theString){
  
  aggregate(get(theString) ~ PROPOSAL_PI_EMPLID,
           data = prepData,
           function(x){ table(as.character(x))[order(table(as.character(x)), decreasing = TRUE)] |> 
               names() |>
               (\(y){y[1]})()
  
  
})
  
}

)

mostCommon <- Reduce(function(x,y){merge(x,y, by = "PROPOSAL_PI_EMPLID", all.x=TRUE)}, mostCommon_list)

names(mostCommon) <- c("PROPOSAL_PI_EMPLID", theStrings)

mostCommon$complex_cluster[is.na(mostCommon$complex_cluster)] <- "unassigned"

mostCommon$rate_cluster[is.na(mostCommon$rate_cluster)] <- "unassigned"

### extract dates

theDates <- aggregate(upload_year ~ PROPOSAL_PI_EMPLID, data = prepData,
function (x) {c(minYr = min(x, na.rm = TRUE),maxYr = max(x, na.rm = TRUE))})

theDates <- cbind(theDates[1], as.data.frame(theDates$upload_year))

theDates$dateRange <- theDates$maxYr - theDates$minYr

# I could pull in something like years since initial hire

# but let's see what this coughs up

### merge back together

fullPI <- merge(fullPI, merge(mostCommon, theDates, by = "PROPOSAL_PI_EMPLID"), by.x = "row.names", by.y = "PROPOSAL_PI_EMPLID" )

row.names(fullPI) <- fullPI$Row.names
names(fullPI)[names(fullPI) %in% "Row.names"] <- "PROPOSAL_PI_EMPLID"

fullPI$class <- ifelse(fullPI$PROPOSAL_PI_EMPLID %in% ageData$EMPLID, "present","missing")

fullPI$class <- factor(fullPI$class)

```


```{r}

## SPLIT

set.seed(42)

splitIndex <- createDataPartition(fullPI$class, p = 0.8, list = FALSE)

trainData <- fullPI[splitIndex,!colnames(fullPI) %in% c("PROPOSAL_PI_EMPLID")]
testData <- fullPI[-splitIndex,!colnames(fullPI) %in% c("PROPOSAL_PI_EMPLID")]

```


```{r eval=FALSE}

## TRAIN

ctrl <- trainControl(
  method = "cv",       # k-fold cross-validation
  number = 5,          # 5-fold CV
  verboseIter = TRUE,  # optional
  classProbs = TRUE,   # needed for AUC/ROC
  summaryFunction = twoClassSummary 
)

set.seed(42)

xgbModel <- train(
  class ~ .,
  data = trainData,
  method = "xgbTree",
  trControl = ctrl,
  metric = "ROC",       # maximize AUC
  preProcess = c("center", "scale"),  # optional
  tuneLength = 5        # try 5 tuning parameter combinations
)

saveRDS(xgbModel, here::here("Robjects", "Identifying missing age values from Dimensions Age Exploratory Data Analysis.rds"))


```

Out of the target population of `r length(unique(prepData$PROPOSAL_PI_EMPLID)) |> format(big.mark=",") ` some `r length(missingSubset) ` (`r missingPercentage`) do not have matches.

Machine learning was used to identify patterns in the missing data by using the available data to classify whether a match in Dimensions data was "missing" or "present."  If no patterns were found, and the missing matches were random, then the missing values could be imputed or the records with missing values removed with less impact on study conclusions. However, the presence of patterns in missing data would undermine study conclusions due to not fully scoping the situation.

Identifying patterns could also help efforts to find matches.   

Extreme gradient boosting was used with default settings to detect whether a PI was "missing" or "present" in Dimensions data using `r ncol(fullPI) - 2` variables, `r sapply(fullPI, class) |> table() |> (\(x){ x[names(x) == "numeric"]})()` numeric and `r sapply(fullPI, class) |> table() |> (\(x){ x[names(x) == "character"]})()` character. 

The most common character value was chosen per PI. The minimum and maximum proposal year (between 2013 and 2023 per population filter criteria), and the range between, was extracted or calculated.  Values like the number of proposals won or lost, and the median amount requested, was calculated per PI.

The machine learning model had an overall accuracy of 76%, precision of 59%, and recall of 34%.  Of most relevance was the Kappa score, a measure of agreement beyond chance, at 0.29 which greatly exceeded a 0.1 threshold.  This suggests the presence of patterns or underlying structure that describe which PI's were missing matches.

### Data set used for machine learning

```{r}

skim(fullPI)

```

### Model performance

```{r}

### PREDICT

filter <- testData$college != "CTSI" &
          testData$PROPOSAL_IACUC_IRB_DIM_KEY != 3

predProbs <- predict(xgbModel, newdata = testData[filter,], type = "prob")
predClass <- predict(xgbModel, newdata = testData[filter,])

drawCM(confusionMatrix(predClass, testData$class[filter]))

# This is such a border-line result
# Weak signals with all that data, but not *nothing*

```



### Variable importance to the machine learning model

```{r}

plot(varImp(xgbModel), 20)


```

### Variables that predict missing matches


```{r}

incomingPar <- par(
  mfrow = c(2,2),
  oma = c(0,0,2,0),
  mar = c(4,4,2,1)
  )

boxplot(maxYr ~ class, data = fullPI,  ylab = "",
        col = c("chocolate4","thistle"),
        main = "Maximum (?) year"
        )

boxplot(dateRange ~ class, data = fullPI, ylab = "",
        col = c("chocolate4","thistle"),
        main = "Date range")

boxplot(log(sum.total) ~ class, data = fullPI, ylab = "", col = c("chocolate4","thistle"), main = "Proposal total (log)")

boxplot(log(count.total) ~ class, data = fullPI, ylab = "", col = c("chocolate4","thistle"),
        main = "Proposal count (log)")

mtext(side = 3, text = "Variables that distinguish missing values", font = 2, cex = 1.619, outer = TRUE)

par(incomingPar)

```

```{r}

incomingPar <- par(
  mfrow = c(2,2),
  oma = c(0,0,2,0),
  mar = c(4,4,2,1)
  )

plotCategory("PROPOSAL_PI_FACULTY_LINE_SORTED")

plotCategory("PROPOSAL_PI_APPOINTMENT_COLLEGE", wanted_label = c("NNN","other"))

# plotCategory("VPR_PROPOSAL_COLLEGE", wanted_label = "OTHER")

plotCategory("PROPOSAL_PI_FACULTY_LINE_SORTED")

plotCategory("college", wanted_label = c("Bus", "other", "SocBeh","Med",  "FinArt", "SCI" ) )

par(incomingPar)

```





```{r eval=FALSE}

# I'm getting a picture of lots of "other"
# Let's look closer into those top 7

boxplot(maxYr ~ class, data = fullPI)
boxplot(dateRange ~ class, data = fullPI)
boxplot(log(sum.total) ~ class, data = fullPI)
boxplot(sum.total ~ class, data = fullPI, log = "y")
boxplot(sum.rate ~ class, data = fullPI)

# This isn't in the model...
boxplot(count.total ~ class, data = fullPI,log = "y")
# ...even though it's a huge difference

plotCategory("PROPOSAL_PI_FACULTY_LINE_SORTED")

plotCategory("PROPOSAL_PI_APPOINTMENT_COLLEGE")

plotCategory("VPR_PROPOSAL_COLLEGE", wanted_label = "OTHER")

plotCategory("college", wanted_label = c("Bus", "other", "SocBeh","Med",  "FinArt", "SCI" ) ) # "Science",

table(fullPI[,c("PROPOSAL_PI_FACULTY_LINE_SORTED", "class")]) |> 
  
  (\(x){ 
    plot(x,
         main = "PROPOSAL_PI_FACULTY_LINE_SORTED",
         col = c("darkcyan","chocolate4"))
        
    })()
  
  


# I wonder if the character name matters? like shorter names are harder to match?

aggregate(PROPOSAL_PI_NAME ~ class, data = prepAgeData, function(x) { mean(nchar(as.character(x)))   })

    class PROPOSAL_PI_NAME
1 missing         17.38025
2 present         16.12802

# I could turn this into a t-test, see if it's signficant,
# and I could re-run my model.

# Next, I wonder who is the most successful and prolific PI that we haven't found?

# should I add "percentiles" back into this --- naa, I've already scaled amounts

# Let's follow Chat and conduct various statistical tests
# to find the ones that differ, even if they don't matter to the model.

```





```{r eval = FALSE}

# sandbox

####################################################
## USE ML TO SEE IF MISSING VALUES HAVE A PATTERN ##
####################################################

# prep

######################
## CALCULATE PER PI ##
######################

fullPI <- calculateWinRates(data = prepData, categoryColumn = "PROPOSAL_PI_EMPLID", functionList = list(mean = mean, median = median)) |> (\(x){ x[[1]] })()
# really starting to hate the list nature and why do I want "call" ?

fullPI$count.total <- apply(fullPI[,c("win.count","loss.count")], 1, sum, na.rm = TRUE )
fullPI$sum.total <- apply(fullPI[,c("win.sum","loss.sum")], 1, sum, na.rm = TRUE )



mostCommon <- aggregate(cbind(PROPOSAL_PURPOSE, PROPOSAL_TYPE, PROPOSAL_COLLEGE) ~ PROPOSAL_PI_EMPLID,
           data = prepData,
           
           function(x){ table(as.character(x))[order(table(as.character(x)), decreasing = TRUE)] |> 
               names() |>
               (\(y){y[1]})() |>
               list()
             
            }
           
           )


# wow aggregating is aggravating
# no wonder dplyr took off

uniqueValues <- sapply(prepData[, sapply(prepData, is.factor), drop = FALSE], nlevels)

theStrings <- c("PROPOSAL_PURPOSE", 
                "PROPOSAL_TYPE", 
                "college",
                "PROPOSAL_RECIPIENT_FUNDING_TYPE",
                "PROPOSAL_FANDA_WAIVER_INDIVATOR",
                "PROPOSAL_COST_SHARE_INDICATOR",
          "PROPOSAL_FANDA_OFF_CAMPUS_RATE_INDICATOR",
                "PROPOSAL_UPLOAD_DATE_FISCAL_YEAR",
                "PROPOSAL_PI_APPOINTMENT_COLLEGE",
                "PROPOSAL_PI_ACADEMIC_RANK",
                "PROPOSAL_PI_TENURE_STATUS",
          "PROPOSAL_PI_FACULTY_CATEGORY",
          "PROPOSAL_PI_FACULTY_SUBCATEGORY",
          "PROPOSAL_PI_ACADEMIC_RANK_LEVEL",
          "PROPOSAL_PI_RANK_SORTED",
          "PROPOSAL_PI_FACULTY_LINE_SORTED",
          "VPR_PROPOSAL_COLLEGE",
          "PROPOSAL_VP",
          "VPR_PROPOSAL_VP",
          "PROPOSAL_SPONSOR_TYPE",
          "PROPOSAL_IACUC_IRB_DIM_KEY",
          "HIGHEST_GOVERNMENT_AGENCY_ACRONYM",
          "NEXT_HIGHEST_GOVERNMENT_AGENCY_ACRONYM",
          "complex_cluster",
          "rate_cluster"
                )

mostCommon_list <- lapply(theStrings, function(theString){
  
  aggregate(get(theString) ~ PROPOSAL_PI_EMPLID,
           data = prepData,
           function(x){ table(as.character(x))[order(table(as.character(x)), decreasing = TRUE)] |> 
               names() |>
               (\(y){y[1]})()
  
  
})
  
}

)

mostCommon <- Reduce(function(x,y){merge(x,y, by = "PROPOSAL_PI_EMPLID", all.x=TRUE)}, mostCommon_list)

names(mostCommon) <- c("PROPOSAL_PI_EMPLID", theStrings)

mostCommon$complex_cluster[is.na(mostCommon$complex_cluster)] <- "unassigned"

mostCommon$rate_cluster[is.na(mostCommon$rate_cluster)] <- "unassigned"

theDates <- aggregate(upload_year ~ PROPOSAL_PI_EMPLID, data = prepData,
function (x) {c(minYr = min(x, na.rm = TRUE),maxYr = max(x, na.rm = TRUE))})

theDates <- cbind(theDates[1], as.data.frame(theDates$upload_year))



```


```{r}

keepColumns <- c(
 "PROPOSAL_PI_EMPLID",                      
 "PROPOSAL_ID",                             
 "win",                                     
 "PROPOSAL_DIRECT_COST",                    
 "PROPOSAL_FA_COST",                        
 "PROPOSAL_TOTAL_SPONSOR_BUDGET",           
 "PROPOSAL_UNIVERSITY_COSTSHARE",           
 "PROPOSAL_3RD_PARTY_COSTSHARE",            
 "PROPOSAL_DIRECT_COST_prop",               
 "PROPOSAL_FA_COST_prop",                   
 "PROPOSAL_UNIVERSITY_COSTSHARE_prop",      
 "PROPOSAL_3RD_PARTY_COSTSHARE_prop",       
 "PROPOSAL_SHORT_TITLE",                    
 "PROPOSAL_TYPE",                           
 "PROPOSAL_PURPOSE",                        
 "PROPOSAL_RECIPIENT_FUNDING_TYPE",         
 "PROPOSAL_COST_SHARE_INDICATOR",           
 "PROPOSAL_FANDA_WAIVER_INDIVATOR",         
 "PROPOSAL_FANDA_OFF_CAMPUS_RATE_INDICATOR",
 "PROPOSAL_UPLOAD_DATE_FISCAL_YEAR",        
 "PROPOSAL_PI_NAME",                        
 "PROPOSAL_PI_APPOINTMENT_DEPT",            
 "PROPOSAL_PI_APPOINTMENT_COLLEGE",         
 "PROPOSAL_PI_ACADEMIC_RANK",               
 "PROPOSAL_PI_TENURE_STATUS",               
 "PROPOSAL_PI_FACULTY_CATEGORY",            
 "PROPOSAL_PI_FACULTY_SUBCATEGORY",         
 "PROPOSAL_PI_ACADEMIC_RANK_LEVEL",         
 "PROPOSAL_PI_RANK_SORTED",                 
 "PROPOSAL_PI_FACULTY_LINE_SORTED",         
 "PROPOSAL_ORG",                            
 "PROPOSAL_DEPT",                           
 "VPR_PROPOSAL_COLLEGE",                    
 "PROPOSAL_VP",                             
 "VPR_PROPOSAL_VP",                         
 "PROPOSAL_SPONSOR_ID",                     
 "PROPOSAL_SPONSOR_NAME",                   
 "PROPOSAL_SPONSOR_TYPE_CODE",              
 "PROPOSAL_SPONSOR_TYPE",                   
 "PROPOSAL_IACUC_IRB_DIM_KEY",              
 "PROPOSAL_SPO_EMPLID",                     
 "PROPOSAL_SPO_NAME",                       
 "PROPOSAL_CREATION_DATE",                  
 "PROPOSAL_OSP_RECEIVED_DATE",              
 "PROPOSAL_OSP_REVIEW_DATE",                
 "PROPOSAL_UPLOAD_DATE",                    
 "PROPOSAL_PROJECT_START_DATE",             
 "PROPOSAL_PROJECT_END_DATE",               
 "PROPOSAL_SPONSOR_DUE_DATE",               
 "HIGHEST_GOVERNMENT_AGENCY_ACRONYM",       
 "NEXT_HIGHEST_GOVERNMENT_AGENCY_ACRONYM",  
 "PROPOSAL_CREATION_DATE.week",             
 "PROPOSAL_CREATION_DATE.month",           
 "PROPOSAL_OSP_RECEIVED_DATE.week",         
 "PROPOSAL_OSP_RECEIVED_DATE.month",        
"PROPOSAL_OSP_REVIEW_DATE.week",           
 "PROPOSAL_OSP_REVIEW_DATE.month",          
 "PROPOSAL_UPLOAD_DATE.week",               
 "PROPOSAL_UPLOAD_DATE.month",              
 "PROPOSAL_PROJECT_START_DATE.week",        
 "PROPOSAL_PROJECT_START_DATE.month",       
 "PROPOSAL_PROJECT_END_DATE.week",          
 "PROPOSAL_PROJECT_END_DATE.month",         
 "PROPOSAL_SPONSOR_DUE_DATE.week",          
 "PROPOSAL_SPONSOR_DUE_DATE.month",         
 "interval.org",                            
 "win.count.prior.org",                     
 "loss.count.prior.org",                    
 "total.count.prior.org",                   
 "priorOrg",                                
 "interval.PI",                             
 "win.count.prior.PI",                      
 "loss.count.prior.PI",                     
 "total.count.prior.PI",                    
 "priorPI",                                 
 "collegeDiffer",                           
 "upload_year",                             
 "upload_fiscal_year",                      
 "big5",                                    
 "PROPOSAL_COLLEGE",                        
"college16_bigInst",                       
 "college",                                 
 "total_cut",                               
 "complex_cluster",                         
 "rate_cluster",                            
 "combined_cluster",                        
 "FIRST_PUB_YR",                            
 "LAST_PUB_YR",                             
 "TOTAL_PUBLICATIONS",                      
"FIRST_PUB_TITLE",                         
"FIRST_PUB_ID",                            
 "FIRST_GRANT_YR",                          
 "LAST_GRANT_YR",                           
 "TOTAL_GRANTS",                            
 "DIM_ID",                                  
 "CONFIDENCE",                              
 "CONFIDENCE_DESC",                         
 "class"
)

# I think I'm going to aggregate and merge, and be a little more thoughtful about 
# what I am using to predict the missing values

# let's get Kaidon working on this

# write.csv(paste0("'", unique(prepAgeData$PROPOSAL_PI_EMPLID[prepAgeData$class == "missing"])), here::here("Robjects", "missingEmplid.csv"))



```

To Do:

Create some simple visualizations of some of the columns.

Find the most prolific/high win rate individuals who are still missing.

Clean up the visualizations that I have.

For my own edification, compare differences per variable and statistical significance.


```{r eval=FALSE}

# SANDBOX

# I should probably make a table of duplicates or something like that.

table(ageData$CONFIDENCE) |>
  plot(lwd = 40,
       xpd = TRUE,
       col= c(viridis::viridis(5),"gray"))

# some kind of count of unique PI's per confidence level?

ratingsPer <- table(unique(ageData[,c("EMPLID", "CONFIDENCE")])) |>
  as.data.frame.matrix() |>
  (\(x){x[,"sum"] <- apply(x, 1, sum); return(x)  })()
  

dim(ratingsPer[ratingsPer$sum >1,]) # 828 7

# huh

# let's look at that 1:1

theDupes <- table(ageData[,c("EMPLID", "DIM_ID")]) |>
  as.data.frame.matrix() |>
    (\(x){apply(x, 1, sum)  })() |>
    (\(x){x[x>1]})()

# perfect . . . 
# now what?

View(ageData[ageData$EMPLID %in% names(theDupes),])

# struggling to make use of this, actually
# It's missing so-o many values

# And, at this point I kinda want their name

View(prepAgeData[prepAgeData$PROPOSAL_PI_EMPLID %in% names(theDupes), c("PROPOSAL_PI_EMPLID", "PROPOSAL_PI_NAME", "CONFIDENCE" )])

# 00020501 is both a "W" and a "1".  Interesting.

# I want to describe what proportion of the ~3000 PI's in my data are covered at which confidence level.

# I want to describe what proportion/count of the Age data
# have no duplicates
# have no duplicates and at which confidence level
# Among the duplicates--
#   How many matches
#   How many matches at which confidence level

length(theDupes)/length(unique(ageData$EMPLID))

barplot(as.matrix(c(length(theDupes), length(unique(ageData$EMPLID)[!unique(ageData$EMPLID) %in% names(theDupes)]  ))))

nonDupes <- unique(ageData$EMPLID)[!unique(ageData$EMPLID) %in% names(theDupes)]  

# I'd like multiple bar plots here in a row
# Next one is non-dupes at which confidence level

table(ageData$CONFIDENCE[ageData$EMPLID %in% nonDupes], useNA = "always")

barplot(as.matrix(table(ageData$CONFIDENCE[ageData$EMPLID %in% nonDupes])), col = viridis::viridis(5)) # stack actually looks bad in this case

barplot((table(ageData$CONFIDENCE[ageData$EMPLID %in% nonDupes])), col = c(viridis::viridis(5),"gray") )


ageData$CONFIDENCE[ageData$EMPLID %in% nonDupes] |>
  table() |>
  (\(x){
    x[names(x) != "1"]
    
  })() |>
  barplot(col = c(viridis::viridis(5)[2:5],"grey") )


```




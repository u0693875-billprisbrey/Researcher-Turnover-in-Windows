---
title: "Retention Exploratory Data Analysis"
author: "Bill Prisbrey"
date: "2025-04-25"
output: html_document
---



```{r include=FALSE}

# What I'm thinking on 6-04-2025
# Since I'm about to get a ton of data from HR, then I'll basically need to write myself a whole 'nuther EDA.
# So while the first EDA was called "Quick'n'Dirty", I'm going to call this one "Intermediate" or something.  "Partial data" ?
# The idea is that I'll wrap this up as a relatively stand-alone report, where it is, however truncated it is, and explain that it's incomplete because HR is making a much more complete data store available.


# ===
# It's hard to re-write the flow of an existing report.
# I think it's easier to copy to a new place and start over.
# But I'll keep working on adapting this current one (with lots of frequent commits.)

# I like my new direction  -- it's like two chapters.
# Chapter 1 is the retention data by itself -- basically cleaning up the "quick'n'dirty" version.
# Chapter 2 is the combination, which will require adapting the functions and will probably tell a pretty different story.

# This will be quicker to produce than a blend, but it also won't be the best story-telling or have the best narrative flow.

# Notes for author

# I am over-using the word "active."  I think I want "active" to mean the period they are "actively" proposing, and the word "current" to mean they are currently employed.  But what about then-current?  I guess I'll use the word "active" for proposing, and "employed" for then-employed.

# More thoughts:  
#  - Kruskal wallis is a nice general purpose first-look test, but I should probably find something more appropriate.
#  - Digging into how the intervals were aggregated should be informative (am I sure I'm doing it right?)


# Thoughts:

# - Re-name this as "exploratory data analysis" and summarize the new data view.  "Skimr" and a couple of histograms.
# - I'll probably uncover some data inconsistencies.
# - Use my new "activePI" function to calculate the number of active PI's every week.
# - Create a dataframe that is week-by-week active PI's
# - Use prepData to align proposing periods with hire or re-hire period EVENTUALLY
# - Lack of an initial termination date could be problematic

```


**PURPOSE:**  The purpose of this document is to describe the incomplete retention data of principal investigators originally available to the Office of Sponsored Projects.  This is a truncated report and demonstrates a stage in progress.  It is useful as a milestone or reference but should not be used to draw conclusions.    

**OBJECTIVES:**   

  1.  Describe the retention data.
  2.  Calculate and compare turnover    
      a.  By colleges    ***DONE***  
      b.  By departments ***NOT DONE***   
      c.  By PI clusters ***DONE***    
      d.  By percentiles ***DONE***    
  3.  Describe the combination of the retention data and the proposal data.   ***ABANDONED IN THIS REPORT***   
  4.  Re-calculate and compare turnover   
      a.  By colleges     ***DONE***      
      b.  By departments  ***NOT DONE***    
      c.  By PI clusters  ***DONE***     
      d.  By percentiles  ***DONE***
  5.  Briefly describe highlights and identify avenues for further exploration and next steps.           




**NOTES FROM REVIEW MEETING:**

This report was reviewed on 4.30.2025 with the following action item and summary.

Action Items:
- Dave to investigate negative and zero values for rehire-to-termination intervals (after Bill sends a list.)
- Dave to investigate:
	- Obtaining information that could describe voluntary and involuntary separation (who was fired vs who quit)
	- Obtaining birthdates from HR (so we can see if the declining number of PI's correlates with old age and retirement.)
- Bill to investigate: 
	- Proposal submission dates against the hire/rehire/termination intervals
	- Number of PI's submitting each year
	- Tighten up interval definitions and aggregations
	- Fix x-axis graph labeling
	- "Tendrils" on the per-week graph of active PI's
	- Compare the mean proposal award and see if it is climbing (to verify the declining number of PI's since COVID.)
	- Compare counts of hire/rehire/termination dates on the same graphic
	- Compare PI's submitting proposals against this view for completion's sake

Summary: 
- Bill asks how the view with the HR data is defined; what defines a PI for inclusion in this view?
- Rehire date is after the termination date in one case, and on the same day for a few others.  Dave will investigate. 	
- The lack of an initial termination date to pair with the initial hire date causes some confusion and guess-work as to when people are actually actively researching.  For example, if someone worked for a semester as a janitor as an undergrad, and then came back 15 years later as a researcher, the data will count her in the denominator of the turnover calculation for those 15 years.  This means the turnover calculation is incorrect.
- Bill wonders:  Can we identify voluntary vs involuntary separation?  --> Dave thinks there might be a "reason for update" field with a code attached to it in PS_JOB.
- The number of active PI's has declined since COVID.  Dave thinks this may be age and retirement.  Dave will try and get the birthday so we can include.
- The turnover has increased every year, dramatically increasing in 2024.
- This is a quick'n'dirty report with some problems:
	- The definition of "active researchers" has problematic assumptions as described above. 
	- The headcount of active researchers (between hire and termination date) by week has "tendrils" that need to be explained.
	- X-axis labels are shifted and interval aggregations need to be double-checked.
	- Kruskal-Wallis is not a very sensitive test nor the most appropriate for a time series
- A visual inspection of the "per cluster" trends shows two clusters moving together, one cluster consistently below the others, and two clusters with volatile and large turn-over.

**CONCLUSIONS AND NEXT STEPS:**

Several aspects of this report were intriguing:   

  - The increasing turnover, especially the spike in 2024       
  - The decline in PI headcount since COVID   
  - The visual inspection of turnover by cluster showing two clusters moving together and one cluster with a consistently lower turnover 
  
As well, many aspects of this report deserve a better treatment, including attempting to eliminate the guess-work introduced by the hire/rehire dates.

Due to these reasons, it was decided to continue investigating turnover by principal investigators.    

  
```{r include = FALSE}

knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message=FALSE, error=FALSE, fig.height=7, fig.width=10)

# adjust graphic parameters

oldPar <- par(cex.main = 3,
    cex.lab = 3,
    cex.axis = 2,
    mar = c(5.1,4.1,4.1,2.1), # default is c(5.1,4.1,4.1,2.1)
    mfrow = c(1,1)
    )

library(lubridate)

```

```{r}

###########
## QUERY ##
###########

# Obtain retention data

keyring::keyring_unlock(keyring = "BIPR", password = "Excelsior!")

library(DBI)
con.ds <- DBI::dbConnect(odbc::odbc(), Driver = "oracle", Host = "ocm-campus01.it.utah.edu", 
                         SVC = keyring::key_list(keyring = "BIPR")[1, 1], UID = keyring::key_list(keyring = "BIPR")[1, 
                                                                                                                    2], PWD = keyring::key_get(keyring = "BIPR", service = keyring::key_list(keyring = "BIPR")[1, 
                                                                                                                                                                                                               1]), Port = 2080)
retentionQuery <- "
SELECT *
FROM VPR.D_PI_EMP_DT_VW EMP_DATES
"

retData <- dbGetQuery(con.ds,
                      retentionQuery)


DBI::dbDisconnect(con.ds)




```


```{r}

##########
## LOAD ##
##########

# PREP SCRIPT

source(here::here("Prep scripts","Adjusting prepData and loading things.R"))


# PER PI

piClusters <- lapply(list.files(here::here("Robjects", "Clustering PIs"), full.names = TRUE), readRDS)

###########
## MERGE ## 
###########

piEmplid <- Reduce(function(x, y) {
  merged <- merge(x, y, by = "emplid", all = FALSE)
  merged <- merged[, !duplicated(sub("\\.x$|\\.y$", "", names(merged)))]  # Remove duplicate columns
  
  # Rename columns to remove ".x"
  names(merged) <- sub("\\.x$", "", names(merged))
  
  merged
}, piClusters)
# piEmplid <- Reduce(function(x, y) merge(x, y, by = "emplid", all = FALSE), piClusters)
# piEmplid <- do.call(merge, piClusters)



prepData <- merge(prepData, piEmplid[,c("emplid","complex_cluster","rate_cluster")], by.x =  "PROPOSAL_PI_EMPLID", by.y = "emplid", all.x = TRUE)

##########
## PREP ##
##########

piEmplid$combined_cluster <-  factor(paste(piEmplid$complex_cluster, piEmplid$rate_cluster, sep = ", "))

prepData$combined_cluster <- factor(paste(prepData$complex_cluster, prepData$rate_cluster, sep = ", "))

###############
## LIBRARIES ##
###############

library(viridis)
library(pheatmap)

#########
## MAP ##
#########

piMap <- data.frame(college = row.names(piEmplid), abbrv = row.names(piEmplid), color = NA, pch = 19, cex = 0.7 )

# I need the college map of colors

fullEmplid <- calculateWinRates(data = cleanData, categoryColumn = "PROPOSAL_PI_EMPLID") |>
  (\(x){x[[1]]})()

fullEmplid$count.total <- apply(fullEmplid[,c("win.count","loss.count")],1,sum)

filterTwoCount <- fullEmplid$count.total >= 2
# filterThreeCount <- piEmplid$count.total > 3


```


```{r}
             
# Manage colors

#############
## COMPLEX ##
#############

complexClusterColors <- c("forestgreen", "deepskyblue", "goldenrod",  
                            "firebrick", "darkslategray", "chartreuse",  
                            "slateblue", "darkkhaki", "coral")

names(complexClusterColors) <- c(as.character(1:5),"unassigned") # I'll surely regret this

clusterComplexMapping <- piMap
clusterComplexMapping[,"pch"] <- rep(19, nrow(clusterComplexMapping))

clusterComplexMapping[,"color"] <- complexClusterColors [piEmplid$complex_cluster]

# clusterComplexMapping[,"color"] <- complexClusterColors[complexHCPC$data.clust$clust[match(clusterComplexMapping[,"college"], row.names(complexHCPC$data.clust))] ]

##############
## COMBINED ##
##############

combinedClusterColors <- c("forestgreen", "deepskyblue", "goldenrod",  
  "firebrick", "darkslategray", "chartreuse",  
  "slateblue", "darkkhaki", "coral",  
  "mediumorchid", "dodgerblue", "tomato",  
  "orchid", "darkseagreen", "sienna",  
  "royalblue", "indianred", "seagreen",  
  "peru", "cadetblue", "plum",  
  "midnightblue", "lawngreen", "darkorange",  
  "lightsteelblue")

clusterCombinedMapping <- piMap
clusterCombinedMapping[,"pch"] <- rep(19, nrow(clusterCombinedMapping))

clusterCombinedMapping[,"color"] <- combinedClusterColors [piEmplid$combined_cluster]



################
## PERCENTILE ##
################

percentileColors <- viridis(10,direction = -1)[c(1:7,10)]
percentileMapping <- piMap

percentileMapping[,"pch"] <- rep(19, nrow(percentileMapping))

percentileMapping[,"color"] <- percentileColors[as.numeric(cut(piEmplid$win.sum_percentile, breaks = seq(0.2,1,by=0.1)))]

#############
## COLLEGE ##
#############

collegeAbbrv <- cbind(
  college = c(sweet16, bigInst, "other"),
  abbrv = c("Arch",
            "Educ",
            "FinArt",
            "Health",
            "Hum",
            "Nurs",
            "Pharm",
            "Science",
            "SocBeh",
            "SocWrk",
            "Bus",
            "Law",
            "Tran",
            "Dent",
            "Med",
            "Engr",
            "EGI",
            "Hunt",
            "SCI",
            "CVRTI",
            "ICSE",
            "CTSI",
            "other"
  ),
  color = c(
    "lightslategray",
    "orange",
    "cyan", 
    "hotpink", 
    "brown", 
    "darkgoldenrod", 
    "gold", 
    "green",
    "navy", 
    "magenta", 
    "olivedrab4", 
    "salmon", 
    "darkgreen",
    "yellowgreen", 
    "red",
    "blue",
    "chocolate", 
    "purple",
    "violet", 
    "khaki",
    "deepskyblue3",
    "chartreuse",
    "darkmagenta"
  ),
  pch = c(
    1,14,15,2,3,4,17,6,5,8,9,10,11,12,13,0,16,7,18, 23, 24, 25, 20  
    
    
  ),
  
  cex = rep(NA, length(c(sweet16, bigInst, "other")) )
)

collegeColors <- setNames(collegeAbbrv[,"color"], collegeAbbrv[,"abbrv"])

```

```{r}

##############################
## ACTIVE PROPOSING PERIODS ##
##############################

minDate <- aggregate(PROPOSAL_UPLOAD_DATE ~ PROPOSAL_PI_EMPLID, data = prepData, FUN =  function(x) {as.Date(min(x))})

maxDate <- aggregate(PROPOSAL_UPLOAD_DATE ~ PROPOSAL_PI_EMPLID, data = prepData, FUN = function(x) {as.Date(max(x))})

activeProposing <- merge(minDate, maxDate, by = "PROPOSAL_PI_EMPLID")
names(activeProposing) <- c("PROPOSAL_PI_EMPLID", "PROPOSAL_UPLOAD_DATE.min", "PROPOSAL_UPLOAD_DATE.max")

activeProposing <- merge(activeProposing, retData[,c("PI_EMPLID", "HIRE_DT", "REHIRE_DT", "TERMINATION_DT")],
               by.x = "PROPOSAL_PI_EMPLID",
               by.y = "PI_EMPLID",
               all.x = TRUE)

```



```{r eval = FALSE}


# Not sure these need their own columns
activeProposing$active <- time_length(interval(activeProposing$PROPOSAL_UPLOAD_DATE.min, activeProposing$PROPOSAL_UPLOAD_DATE.max), unit = "year")

condition <- is.na(activeProposing$TERMINATION_DT)
activeProposing$inactive_to_date[condition] <- time_length(interval(activeProposing$PROPOSAL_UPLOAD_DATE.max[condition], ymd("2025-05-01")), unit = "year")

activeProposing$inactive_to_term <- time_length(interval(activeProposing$PROPOSAL_UPLOAD_DATE.max, activeProposing$TERMINATION_DT), unit = "year")

# what intervals are interesting?
# hire-to-propose
# re-hire-to-propose
# last propose to term
# active-yet-inactive (yrs since last propose for current headcount)
# productive period (first to last publication)

# I need to differentiate "active"-ly proposing and "current"-ly headcount
# maybe I use "productive"

# I need to use the same logic and count the number of "productive PI's" per week
# And then maybe adjust for those who are current and will probably put out a new proposal

# I'd like to see when the "active" period is during a prof's career.  Are they done at age 50?

# And I can figure out the correct period to use by comparing the min proposing date and the max hire/rehire date that is less than that.

# Before I do that, though, I should compare the various intervals.



```


```{r}

##############################
## PICK HIRE OR REHIRE DATE ##
##############################

# I want:
#  Were they proposing between their hire and re-hire date?  Only before their re-hire date?
#  Were they proposing only after their re-hire date?
#  Were they proposing before AND after their re-hire date?

before_rehire <- activeProposing$PROPOSAL_UPLOAD_DATE.max <= activeProposing$REHIRE_DT

after_rehire <- activeProposing$PROPOSAL_UPLOAD_DATE.min >= activeProposing$REHIRE_DT

before_and_after <- activeProposing$PROPOSAL_UPLOAD_DATE.min <= activeProposing$REHIRE_DT &  activeProposing$PROPOSAL_UPLOAD_DATE.max >= activeProposing$REHIRE_DT
 
# I kinda wanna see a "hire timeline"
# similar to my timeline and timedots, but with hire/rehire/term marked
# and scale it to actual years


#> table(before_rehire)
#before_rehire
#FALSE  TRUE 
#  634   162 
#> table(after_rehire)
#after_rehire
#FALSE  TRUE 
#  191   605 
#> table(before_and_after)
#before_and_after
#FALSE  TRUE 
#  767    29 

# Check
#> table(before_rehire & after_rehire)
#
#FALSE 
#  796 
#> table(before_rehire & before_and_after)
 
#FALSE 
#  796 
#> table(after_rehire & before_and_after)

#FALSE 
#  796 

activeProposing$effective_hire <- activeProposing$HIRE_DT

rehire_condition <- after_rehire & !is.na(after_rehire)
activeProposing$effective_hire[rehire_condition] <- activeProposing$REHIRE_DT[rehire_condition] 

  
```


```{r}

# modified timeline
# I want to modify the timeline to plot against the years
# and include the hire/rehire/termination dates

# a good picture is worth a thousand words.



```


# EXECUTIVE SUMMARY

All graphics produced in this report used incomplete data, and no conclusions should be drawn.

This data shows a decline in the estimated headcount of principal investigators starting in about 2020 due to reduced hires and increased departures.  

It also shows that the population above the 90th percentile for funds won are more stable with fewer departures than the rest.

I need to conclude about the clusters

Upon re-analyzing with complete data, valid conclusions can be drawn.

# SUMMARY

The retention data contains records for `r nrow(retData) |> format(big.mark = ",")` principal investigators, where principal investigators are extracted from the table "osp.d_pi_vw" and are presumably identified according to the definitions and designations found in (Rule R7-200B)[https://regulations.utah.edu/research/rules_7/r7-200b.php#a.II].  The definition of a principal investigator and inclusion in this data set is being reviewed.

This data set includes all `r table(retData$PI_EMPLID %in% prepData$PROPOSAL_PI_EMPLID)[2] |> format(big.mark = ",") ` principal investigators who submitted proposals after FY2013 as described in the Grants Exploratory project, and contains an additional `r table(retData$PI_EMPLID %in% prepData$PROPOSAL_PI_EMPLID)[1] |> format(big.mark = ",") ` principal investigators who presumably submitted proposals before 2013.  Although additional information from HR is forthcoming, and the designation of "principal investigator" in OSP data is being reviewed, it may be prudent to exclude the additional PI's from future projects due to incomplete data on their proposal submissions.

The data contains up to three dates per PI:  initial hire date, one re-hire date, and the most recent termination date.  Because not all termination and re-hire dates are included, it is impossible to accurately tabulate head count.  This prevents accurate calculation of metrics that use the headcount as a denominator, such as the turnover rate.  As well,action reasons, such as an explanation for termination, are not included, making it is impossible to distinguish "voluntary" and "involuntary" separation.


The earliest hire date reaches back to  `r year(min(retData$HIRE_DT, na.rm = TRUE))`, and the earliest termination date is in `r year(min(retData$HIRE_DT, na.rm = TRUE))`.  The largest interval between initial hire and rehire date is `r round(max(retData$initial, na.rm = TRUE))` years, and the largest interval between hire date and termination date is `r round(max(retData$hire, na.rm = TRUE))` years.

The data shows strong seasonality, with most hire or termination activity happening around July 1st.

Head count gradually 





The count of hires is fairly stable after 2005 until declining 

Describe that this is an investigation into turnover of principal investigators.

Describe how principal investigators are defined; and reference some of the open questions that we're checking.


Describe some details of the data (earliest hire date, number of PI's.)

Describe some quirks (longest spans between hire and rehire, hire and termination, rehire and termination.)

At the time of the preparation of this report, the Office of Sponsored Projects had access to incomplete HR data.  Although hire, departure, and net delta counts are possible, the head count had to be estimated, preventing accurate calculation of turnover rates.  Additionally, due to a lack of action reasons, voluntary and involuntary separation could not be differentiated.

Generally, the data set shows an increase in estimated headcount followed by a decline starting in 2020.  The College of Medicine has the largest headcount of PI's, and the steepest decline.  Huntsman Cancer Institute had a modest decline in head count starting in 2024.

The PI's greater than the 90th percentile in funds requested won showed stability, with equivalent hires and fewer departures than PI's greater than the 70th or 80th percentile.

I need to change everything to counts, and only interpret that.

Population definition -- who got in here and why?






It consisted of a an intial hire date and re-hire or final termination dates as applicable.  Because there was no initial termination date paired with the re-hire date, the re-hire date was ignored.  This means that the headcount was calculated only using the initial hire date.  Because the head count was wrong, metrics like turnover rate that rely on the head count are wrong.



# HIGHLIGHTS, AVENUES FOR FURTHER EXPLORATION, AND NEXT STEPS





#1) DESCRIBE THE RETENTION DATA   

Put a few words here describing the query.

Put a few words here describing "skimr" results -- number of rows, PI's, hires, re-hires, and terminations.

Then explain the handful of graphics that I want to show here.  Show the graphics under their own headings.

### Retention dates

```{r}

# Raw numbers, yearly, individual graphics

plotPar <- par(mfrow = c(3,1), 
               bg = "ivory", 
               fg = "gray20",
               mar = c(2, 4.1, 2, 0.3)
               )

#mar = c(2, 4.1, 1.1, 0.1)) # c(5.1,4.1,4.1,2.1)

table(year(retData$HIRE_DT)) |>
  (\(x){ 
    x[names(x) != "NA-NA"]
    })() |>
  plot(ylab = "Count of hires",
       main = "Hires over time",
       type = "l",
       col = "chocolate4")


table(year(retData$REHIRE_DT)) |>
  (\(x){ 
    x[names(x) != "NA-NA"]
    })() |>
  plot(ylab = "Count of re-hires",
       main = "Re-hires over time",
       type = "b",
       col = "chocolate4")



table(year(retData$TERMINATION_DT)) |>
  (\(x){ 
    x[names(x) != "NA-NA"]
    })() |>
  plot(ylab = "Count of terminations",
       main = "Terminations over time",
       type = "b",
       col = "chocolate4")



par(plotPar)

# maybe a cumulative graphic?
# maybe scaled lines on the same graphic? (Gotta do this one!)

```



### Retention intervals

```{r}

#########################
## CALCULATE INTERVALS ##
#########################

# library(lubridate)

# This is the initial time between "hire date" and "re-hire date"
retData$initial <- time_length(interval(retData$HIRE_DT, retData$REHIRE_DT), unit = "year")

# This is the time between "re-hire date" and "termination date"
retData$rehire <- time_length(interval(retData$REHIRE_DT, retData$TERMINATION_DT), unit = "year")

# This is the time from initial hire to termination date
retData$hire <- time_length(interval(retData$HIRE_DT, retData$TERMINATION_DT), unit = "year")


```

```{r}

#######################
## DISPLAY INTERVALS ##
#######################

histPar <- par(mfrow = c(3,1), mar = c(2.5, 4.1, 2.6, 0.1)) # c(5.1,4.1,4.1,2.1)

hist(retData$initial,
     main = "Interval in years after hire until rehire",
     cex.main = 1.382,
     ylab = "Count",
     xlab = "", # "Duration in years",
     col = "lightseagreen")
legend("topright", legend = paste( format(sum(is.na(retData$initial)), big.mark = ","), "NA values"), bty = "n", text.col = "red", cex = 1.618)

hist(retData$rehire,
     main = "Interval in years after rehire until termination",
     cex.main = 1.382,
     ylab = "Count",
     xlab = "", # "Duration in years",
     col = "lightgreen")
legend("topright", legend = paste(format( sum(is.na(retData$rehire)), big.mark=",") , "NA values"), bty = "n", text.col = "red", cex = 1.618)


hist(retData$hire,
     main = "Interval in years after hire until termination",
     cex.main = 1.382,
     ylab = "Count",
     xlab = "", # "Duration in years",
     col = "aquamarine")
legend("topright", legend = paste(format( sum(is.na(retData$hire)), big.mark= ","), "NA values"), bty = "n", text.col = "red", cex = 1.618)

par(histPar)

```

Duration thoughts:    

  - The lack of a termination date before the re-hire date introduces guess-work.   
  - Some researchers are apparently hired as students and re-hired later in their career.    
  - Some researchers are apparently hired in retirement after their career. 
  - These durations need to be aligned with proposal submission dates.    
  - This shows that we currently have 2,994 active principal investigators.   

### Seasonality

```{r}

# I like this graphic.
# Convert it to three lines for my three date columns.
# Maybe move this after "dates" and before "intervals."

weeklyTerms <- table(week(retData$TERMINATION_DT))
weeklyHires <- table(week(retData$HIRE_DT))
weeklyRehires <- table(week(retData$REHIRE_DT))

yLim <- c(min(c(weeklyTerms, weeklyHires,weeklyRehires), na.rm = TRUE), max(c(weeklyTerms, weeklyHires,weeklyRehires), na.rm = TRUE) )

plotPar <- par(bg = "ivory", fg = "gray20")

plot(weeklyTerms,
     ylim = yLim,
     type = "n",
     ylab = "",
     las = 1,
     xlab = "week of year")

points(weeklyHires,
       type = "l",
       col = "purple")

points(weeklyRehires,
       type = "l",
       col = "orange")

points(weeklyTerms,
       type = "l",
       col = "firebrick")

mtext(side = 3,
      "Workforce flow has strong seasonality\nwith most activity happening in Week 26",
      font =2,
      cex = 1.384,
      line = 1)

legend("topleft",
       legend = c("hire","rehire","termination"),
       col = c("purple","orange", "firebrick"),
       pch = 15,
       pt.cex = 2)

par(plotPar)



table(week(retData$TERMINATION_DT)) |>
  plot(xlab = "week of year",
       ylab = "Count of terminations per week",
       main = "PIs are mostly terminated around June 30th")



```

```{r eval=FALSE}

# I decided to use the annual one instead (converted to a line chart)

# Raw numbers, quarterly, individual graphics

plotPar <- par(mfrow = c(3,1), bg = "ivory", fg = "gray20")

table(paste(year(retData$HIRE_DT), quarter(retData$HIRE_DT), sep = "-")) |>
  (\(x){ 
    x[names(x) != "NA-NA"]
    })() |>
  plot(ylab = "Count of hires",
       main = "Hires over time",
       col = "chocolate4")


table(paste(year(retData$REHIRE_DT), quarter(retData$REHIRE_DT), sep = "-")) |>
  (\(x){ 
    x[names(x) != "NA-NA"]
    })() |>
  plot(ylab = "Count of re-hires",
       main = "Re-hires over time",
       col = "chocolate4")



table(paste(year(retData$TERMINATION_DT), quarter(retData$TERMINATION_DT), sep = "-")) |>
  (\(x){ 
    x[names(x) != "NA-NA"]
    })() |>
  plot(ylab = "Count of terminations",
       main = "Terminations over time",
       col = "chocolate4")



par(plotPar)

# maybe a cumulative graphic?
# maybe scaled lines on the same graphic? (Gotta do this one!)

```



#2) CALCULATE AND COMPARE TURNOVER

### Active principal investigators

Except this is the date between "hire" and "rehire", so it's not accurate. 

"Active" is not a good term.

```{r}

########################
## CALCULATE TURNOVER ##
########################

calendar <- c("day","week", "month", "quarter", "year")

fullSpan <- lapply(calendar, function(x){
  
  calculateMetrics(calendar = x,
                   minDate = ymd("1960-01-01"),
                   maxDate = today(),
                   data = retData)
  
})
names(fullSpan) <- calendar

midSpan <- lapply(calendar, function(x){
  
  calculateMetrics(calendar = x,
                   minDate = ymd("2013-01-01"),
                   maxDate = today(),
                   data = retData)
  
})
names(midSpan) <- calendar

earlySpan <- lapply(calendar, function(x){
  
  calculateMetrics(calendar = x,
                   minDate = ymd("2000-01-01"),
                   maxDate = ymd("2020-12-31"),
                   data = retData)
  
})
names(earlySpan) <- calendar

lateSpan <- lapply(calendar, function(x){
  
  calculateMetrics(calendar = x,
                   minDate = ymd("2020-01-01"),
                   maxDate = today(),
                   data = retData)
  
})
names(lateSpan) <- calendar

```

```{r}

plotMetrics(
  data = fullSpan[["year"]],
  plotList = c("cumulative","count","delta.count"),
  title_mtext_params = list(text = "Approximate PI Headcount\nrehire dates ignored", line = -1.618),
  cumulative_plot_params = list(mar = c(0,6,3,1))
)


plotMetrics(
  data = midSpan[["year"]],
  plotList = c("cumulative","count","delta.count"),
    title_mtext_params = list(text = "Approximate PI Headcount\nrehire dates ignored", line = -1.618),
  cumulative_plot_params = list(mar = c(0,6,3,1))
)

```


I need some words like

[x] number of PI's have hire and rehire dates.  This creates uncertainty in the data as there is only one termination date.

Therefore, only the hire date is used.  And the re-hire date is completely ignored.

This means that individuals are counted after their hire date.  "Headcount" includes individuals who are after their hire date and before their termination date.

It means I am including someone who might have worked as a janitor during their undergraduate years, quit upon graduation, and have been elsewhere.

It also means that I am including someone who 

(room with hallway graphic (?))

Let's show a graphic with the re-hires removed completely.

I really need to emphasize my uncertainty with this data and graphic.  There's a strong guess here, or this reflects an assumption that is wrong.

Probably a discussion on how my hire and departure rates are calculated on an avg headcount basis.

And, what if this is COMPLETELY wrong?  Like, laughably and insanely wrong?

It might be a good time to walk through the SQL logic, too.

I should pull up a specific example of where I think they were a student, and where they were re-hired after their career.

```{r eval = FALSE}

# Removing the re-hires
# I don't think there's a point in showing this graphic.

calculateMetrics(calendar = "year",
                 minDate = ymd("2013-01-01"),
                 maxDate = today(),
                 data = retData[is.na(retData$REHIRE_DT),]) |>
  (\(x){
  plotMetrics(
      data = x,
  plotList = c("cumulative","count","delta.count"),
    title_mtext_params = list(text = "Approximate PI Headcount\nrehires removed", line = -1.618),
  cumulative_plot_params = list(mar = c(0,6,3,1))
  )})()


```

```{r eval=FALSE}

########################
## CALCULATE TURNOVER ##
########################

turnover_wk <- calculateTurnover(data = retData, interval = "week")

turnover_qt <- calculateTurnover(data = retData, interval = "quarter")

turnover_yr <- calculateTurnover(data = retData, interval = "year")


```

```{r eval=FALSE}

# delete me sooner rather than later

plotPar <- par(mfrow = c(3,1), 
               bg = "ivory", 
               fg = "gray20",
               mar = c(4, 4.1, 2, 0.3)
               )

plot(turnover_wk$hire,
     cex = 0.3,
     main = "Count of PI's between hire and termination date",
     col="sienna",
     ylab = "Count of active PIs",
     xlab = "",
     xaxt = "n")

ticks = seq(from = 0, to = nrow(turnover_wk), length.out = 12)
axis(side = 1,
     at = ticks,
     labels = turnover_wk$label[ticks+1],
     las = 2)

plot(turnover_qt$hire,
     cex = 0.3,
     main = "Count of PI's between hire and termination date",
     col="sienna",
     ylab = "Count of active PIs",
     xlab = "",
     xaxt = "n")

ticks = seq(from = 0, to = nrow(turnover_qt), length.out = 12)
axis(side = 1,
     at = ticks,
     labels = turnover_qt$label[ticks+1],
     las = 2)

plot(turnover_yr$hire,
     cex = 0.3,
     main = "Count of PI's between hire and termination date",
     col="sienna",
     ylab = "Count of active PIs",
     xlab = "",
     xaxt = "n")

ticks = seq(from = 0, to = nrow(turnover_yr), length.out = 12)
axis(side = 1,
     at = ticks,
     labels = turnover_yr$label[ticks+1],
     las = 2)



```


[FIXED discussion points]
  - Some aspects of this graph appear to be mis-aligned due to how counts per interval were aggregated:
    - Axis labels  
    - Points not along the main curve   
    - This needs to be investigated   
    
[RELEVANT discussion points]    
  - The general pattern of an increasing count of PI's that peaked around the year 2020 and declined since needs to be confirmed    
  - Because the total money requested won has increased, this would mean the average per PI has increased.  This could be one double-check. 


```{r eval=FALSE}

# I can delete this
# investigating the "tendrils"

plotPar <- par(mfrow = c(1,1), 
               bg = "ivory", 
               fg = "gray20",
               mar = c(4, 4.1, 4, 0.3)
               )

dateFilter <- turnover_wk$termDT > as.Date("2018-12-01") & turnover_wk$termDT <= as.Date("2020-06-01")

range(scale(turnover_wk$exit[dateFilter]))

plot(scale(turnover_wk$hire[dateFilter]),
     cex = 0.3,
     main = "Count of PI's between hire and termination date \ndoesn't align with exits",
     #main = "Count of PI's between hire and termination date",
     col="sienna",
     ylab = "Count of active PIs",
     xlab = "",
     xaxt = "n",
     type = "l",
     ylim = c(-2,9)
     )

# that's a little clearer what's going on.
# looks like these tendrils simply align with terminations and hires

# So now I'm trying to align them graphically

points(scale(turnover_wk$exit[dateFilter]),
     cex = 0.3,
     # main = "Count of PI's terminated",
     type = "l",
     col="red",
     ylab = "Count of terminated PIs",
     xlab = "",
     xaxt = "n")

legend("topright",
       legend = c("head count", "exit count"),
       lty = 1,
       lwd = 2,
       col = c("sienna","red"),
       cex = 1.3)

#ticks = seq(from = 0, to = nrow(turnover_wk), length.out = 12)
#axis(side = 1,
#     at = ticks,
#     labels = turnover_wk$label[ticks+1],
#     las = 2)

mtext(side = 1, line = 1.33, "This graph shows weekly calculations between 2018-12-01 and 2020-06-01")

mtext(side = 1, line = 2.33, "The misalignment suggests an error in the aggregating formula.")

par(plotPar)



```

  

```{r eval=FALSE}


plotPar <- par(bg = "ivory", fg = "gray20")

plot(turnover_wk$exit,
     cex = 0.3,
     main = "Count of PI's terminated",
     type = "l",
     col="darkorange2",
     ylab = "Count of terminated PIs",
     xlab = "",
     xaxt = "n")

ticks = seq(from = 0, to = nrow(turnover_wk), length.out = 12)
axis(side = 1,
     at = ticks,
     labels = turnover_wk$label[ticks+1],
     las = 2)




```

```{r eval=FALSE}

qt_sc <- turnover_qt[,-(1:2)] |>
  scale()

yr_sc <- turnover_yr[,-(1:2)] |>
  scale()


```


```{r eval=FALSE}

plot(1,
     ylim = c(min(qt_sc),max(qt_sc)),
     xlim = c(0,nrow(qt_sc)),
     type = "n",
     xaxt = "n",
     xlab = "",
     ylab = ""
)

lines(qt_sc[,1], col = "sienna")
lines(qt_sc[,3], col = "darkorange2")

legend("topleft",
       legend = c("Active PIs","Turn-over"),
       col = c("sienna","darkorange2"),
       lty = 1,
       lwd = 1.619)

mtext(side = 3,
      "Quarterly PI head-count and turnover\n(scaled)",
      line = 1.33,
      cex=1.3,
      font = 2)

ticks <- seq(from = 0, to = nrow(turnover_qt), length.out = 9)
axis(side = 1,
     at = ticks,
     las =2,
     labels = turnover_qt[ticks+1 ,"label"])



```

```{r eval=FALSE}

# because the turnover is in error (based on assumptions we know are faulty) then there's not so much point in emphasizing or zooming in on this graphic or scaling it with the headcount.

# but the increasing departure rate is an interesting talking point.

plot(1,
     ylim = c(min(yr_sc),max(yr_sc)),
     xlim = c(0,nrow(yr_sc)),
     type = "n",
     xaxt = "n",
     xlab = "",
     ylab = ""
)

lines(yr_sc[,1], col = "sienna", lwd = 2)
lines(yr_sc[,3], col = "darkorange2")

legend("topleft",
       legend = c("Active PIs","Turn-over"),
       col = c("sienna","darkorange2"),
       lty = 1,
       lwd = 1.619)

mtext(side = 3,
      "Yearly PI head-count and turnover\n(scaled)",
      line = 1.33,
      cex=1.3,
      font = 2)

ticks <- seq(from = 0, to = nrow(turnover_yr), length.out = 9)
axis(side = 1,
     at = ticks,
     las =2,
     labels = turnover_yr[ticks+1 ,"label"])


```

Discussion points:    

  - Turnover is calculated as the number of exits divided by the average head count of active researchers per period.
  - This calculation and graphic needs the same due diligence as noted previously (axis labels shifted and double-check of interval aggregations.)    
  - The general trend of increasing turnover, spiking last year, is somewhat alarming.  It may be worthwhile to compare PI turnover to turnover by all faculty.   

# TURNOVER BY COLLEGE

I'm struggling if there's a point to do this.

Maybe just to develop the mechanics?

```{r}

colleges <- unique(prepData$college)
collegePIs <- lapply(colleges, function(x) unique(prepData$PROPOSAL_PI_EMPLID[prepData$college == x]))
names(collegePIs) <- colleges

collegeMetrics <- lapply(collegePIs, function(x){
  
  calculateMetrics(data=retData[retData$PI_EMPLID %in% x,],
                   minDate = ymd("2013-01-01"),
                   maxDate = today(), 
                     calendar = "year")
  
})


```


```{r eval=FALSE}

colleges <- unique(prepData$college)
collegePIs <- lapply(colleges, function(x) unique(prepData$PROPOSAL_PI_EMPLID[prepData$college == x]))
names(collegePIs) <- colleges

collegeTurnover <- lapply(collegePIs, function(x){
  
  calculateTurnover(data=retData[retData$PI_EMPLID %in% x,] , interval = "year")
  
})

```


```{r eval=FALSE}

# Waiting on better data with fewer assumptions

# Kruskal-wallis

cT <- unlist( lapply(collegeTurnover, function(df) df[["to"]]))

kruskal.test(cT ~ names(cT))

cT.frame <- do.call(rbind, collegeTurnover)
cT.frame$college <- sub("\\..*", "", row.names(cT.frame))

college.kW <- kruskal.test(to ~ college, data = cT.frame)

```


```{r eval=FALSE}

# Do this with the adjusted data,
# or better data if you get it (that doesn't ignore "rehire" date)

plot(1, 
     type = "n", 
     ylim = c(0.,0.3),  #c(-4,4), 
     xlim = c(2013,2026),
     xaxt = "n",
     xlab = "",
     ylab = "turnover")
lapply(collegeTurnover, function(x){
  lines((x[,"to"]), x = as.numeric(x[,"label"]), col = "gray40")
  
})

ticks <- seq(from = 2013, to = 2026, by = 2)
axis(side = 1,
     at = ticks,
     las =2,
     labels = ticks)

legend("topleft", legend = paste("p-value of", round(college.kW$p.value,2)), text.col = "red", bty = "n")

mtext(side = 3,
      text = "Turnover does not vary greatly by college",
      cex = 1.3,
      line= 2,
      font = 2)

mtext(side = 3,
      text = "(according to the Kruskal-Wallis test)",
      cex = 1,
      line = 0.5,
      font = 3)


```


Discussion points:    

  - Why do these lines end at different points?
  - This is a good graphic for plotly (enables hover-over)
  - Kruskal-Wallis test is a pretty rough check   
# TURNOVER BY COMPLEX CLUSTER


```{r}

complex_clusters <- levels(prepData$complex_cluster)

complexPIs <- lapply(complex_clusters, function(x) unique(prepData$PROPOSAL_PI_EMPLID[prepData$complex_cluster == x & !is.na(prepData$complex_cluster)]))
names(complexPIs) <- complex_clusters

# add the unassigned PI's, or NA values for prepData$complex_cluster

complexPIs[["unassigned"]] <- unique(prepData$PROPOSAL_PI_EMPLID[is.na(prepData$complex_cluster)])

complexMetrics <- lapply(complexPIs, function(x){
  
  calculateMetrics(data=retData[retData$PI_EMPLID %in% x,],
                    minDate = ymd("2013-01-01"),
                    maxDate = today(), 
                    calendar = "year")
  
})

```

```{r eval=FALSE}

# repeat with better data that doesn't ignore "rehire" date

# Kruskal-wallis

compT.frame <- do.call(rbind, complexTurnover)
compT.frame$complex_cluster <- sub("\\..*", "", row.names(compT.frame))

complex.kW <- kruskal.test(to ~ complex_cluster, data = compT.frame)

```


```{r eval=FALSE}

# re-do this with plotly and the correct cluster colors

plot(1, 
     type = "n", 
     ylim = c(0.,0.1),  #c(-4,4), 
     xlim = c(2013,2026),
     xaxt = "n",
     xlab = "",
     ylab = "turnover")
lapply(complexTurnover, function(x){
  lines((x[,"to"]), x = as.numeric(x[,"label"]), col = "gray40")
  
})

ticks <- seq(from = 2013, to = 2026, by = 2)
axis(side = 1,
     at = ticks,
     las =2,
     labels = ticks)

legend("topleft", legend = paste("p-value of", round(complex.kW$p.value,2)), text.col = "red", bty = "n")

mtext(side = 3,
      text = "Turnover does not vary greatly by cluster",
      cex = 1.3,
      line= 2,
      font = 2)

mtext(side = 3,
      text = "(according to the Kruskal-Wallis test)",
      cex = 1,
      line = 0.5,
      font = 3)


```

```{r eval=FALSE}

  p <- plot_ly(
    type = "scatter",
    mode = "lines",
    xaxis = list(title = "", range = c(2013,2026)),
    yaxis = list(title = "Departure rate", range = c(0,0.1))
  )
  
for(x in complexMetrics){
    
     p <- add_lines(p, 
                   x = as.numeric(x[, "adjDate"]),
                   y = 100*x[, "termRate"],
                   line = list(color = "gray40"),
                   showlegend = FALSE)
    
  }

# Needs a title
# Needs cluster colors
# Without reference to the cluster, it's pretty pointless
# This really doesn't need to be a "plotly" graphic.
# I can do this with the correct colors and a good legend that has the cluster nick-name.



```

```{r}

#########################
##        PLOT OF      ##
## METRICS PER CLUSTER ##
#########################

plotMetrics(data = complexMetrics,
            plotList = c("cumulative","count","delta.count"),
            term_points_params = list(type = "l", lwd=2),
            hire_points_params = list(type = "l"),
            metric_legend_params = list(x="topleft"),
            delta_legend_params = list(plot=FALSE),
            featureMap = complexClusterColors,
            title_mtext_params = list(text = "Head count metrics per cluster")
            )


#########################
##        PLOT OF      ##
## METRICS PER UNASSIGNED CLUSTER ##
#########################

plotMetrics(data = complexMetrics["unassigned"],
            plotList = c("cumulative","count","delta.count"),
            term_points_params = list(type = "l", lwd=2),
            hire_points_params = list(type = "l"),
            metric_legend_params = list(x="topleft"),
            delta_legend_params = list(plot=FALSE),
            featureMap = complexClusterColors,
            title_mtext_params = list(text = "Head count metrics per cluster")
            )

#########################
##        PLOT OF      ##
## METRICS PER 2 and 4 CLUSTER ##
#########################

plotMetrics(data = complexMetrics[c(2,4)],
            plotList = c("cumulative","count","delta.count"),
            term_points_params = list(type = "l", lwd=2),
            hire_points_params = list(type = "l"),
            metric_legend_params = list(x="topleft"),
            delta_legend_params = list(plot=FALSE),
            featureMap = complexClusterColors,
            title_mtext_params = list(text = "Head count metrics per clusters 2 and 4")
            )


#########################
##        PLOT OF      ##
## METRICS PER 1 and 3 CLUSTER ##
#########################

plotMetrics(data = complexMetrics[c(1,3)],
            plotList = c("cumulative","count","delta.count"),
            term_points_params = list(type = "l", lwd=2),
            hire_points_params = list(type = "l"),
            metric_legend_params = list(x="topleft"),
            delta_legend_params = list(plot=FALSE),
            featureMap = complexClusterColors,
            title_mtext_params = list(text = "Head count metrics per clusters 1 and 3")
            )


#########################
##        PLOT OF      ##
## METRICS PER 5 CLUSTER ##
#########################

plotMetrics(data = complexMetrics[5],
            plotList = c("cumulative","count","delta.count"),
            term_points_params = list(type = "l", lwd=2),
            hire_points_params = list(type = "l"),
            metric_legend_params = list(x="topleft"),
            delta_legend_params = list(plot=FALSE),
            featureMap = complexClusterColors,
            title_mtext_params = list(text = "Head count metrics per cluster 5")
            )

```


```{r}

#########################
##        PLOT OF      ##
## METRICS PER COLLEGE ##
#########################

# Medicine

plotMetrics(data = collegeMetrics[c("Med")],
            plotList = c("cumulative","count","delta.count"),
            term_points_params = list(type = "l", lwd=2),
            hire_points_params = list(type = "l"),
            metric_legend_params = list(x="left"),
            delta_legend_params = list(x="left"),
            featureMap = collegeColors,
            title_mtext_params = list(text = "Head count metrics (Medicine)")
            )

# Next three

plotMetrics(data = collegeMetrics[c("Hunt","Engr","Science")],
            plotList = c("cumulative","count","delta.count"),
            term_points_params = list(type = "l", lwd=2),
            hire_points_params = list(type = "l"),
            metric_legend_params = list(x="left"),
            delta_legend_params = list(x="bottomleft"),
            featureMap = collegeColors,
            title_mtext_params = list(text = "Head count metrics (three large organizations)")
            )

# Everyone else

plotMetrics(data = collegeMetrics[!names(collegeMetrics) %in% c("Med", "Hunt","Engr","Science")],
            plotList = c("cumulative","count","delta.count"),
            term_points_params = list(type = "l", lwd=2),
            hire_points_params = list(type = "l"),
            metric_legend_params = list(x = "top"),
            metric_legend2_params = list(plot = FALSE),
            delta_legend_params = list(plot = FALSE),
            featureMap = collegeColors,
            title_mtext_params = list(text = "Head count metrics (remaining organizations)")
            )

# DOES TERMINATION ALWAYS START AT ZERO? COUNT AND RATE?
# OR JUST FOR THIS DATA SELECTION?

```


Discussion points:    

  - Why do these lines end at different points?
  - This is a good graphic for plotly (enables hover-over)
  - Kruskal-Wallis test is a pretty rough check
    * It isn't taking the time series into account
  - I'd like to add the cluster colors

# TURNOVER BY PERCENTILES

```{r}

###########################
## CALCLUATE PERCENTILES ##
###########################

# I'm writing over my method for the complex_cluster.
# So you may be looking at a mix of new and old code here

# now I am using piClusters[[2]] instead of piEmplid, as it has -all- PI's without the filtering based on count

piClusters[[2]][,"win.sum_tile"] <- cut(piClusters[[2]][,"win.sum_percentile"], breaks = c(0,seq(0.2,1,by=0.1)), include.lowest = TRUE )

piPercentiles <- levels(piClusters[[2]][,"win.sum_tile"])

pisPerTile <- lapply(piPercentiles, function(x) {
  filter <- piClusters[[2]][,"win.sum_tile"] == x
  unique(piClusters[[2]][filter,"emplid"])
  
  })

names(pisPerTile) <- piPercentiles

# Not needed as I am using -all- PI's to calcualte the percentile
# add the unassigned PI's, or NA values for prepData$complex_cluster

# pisPerTile[["unassigned"]] <- unique(prepData$PROPOSAL_PI_EMPLID[!(prepData$PROPOSAL_PI_EMPLID %in% unlist(pisPerTile))])

tileMetrics <- lapply(pisPerTile, function(x){
  
  calculateMetrics(data=retData[retData$PI_EMPLID %in% x,],
                    minDate = ymd("2013-01-01"),
                    maxDate = today(), 
                    calendar = "year")
  
})

```


```{r}

######################
## PLOT PERCENTILES ##
######################

tileColors <- c("burlywood1", percentileColors)
names(tileColors) <- names(tileMetrics)

plotMetrics(tileMetrics,
            plotList = c("cumulative","count","delta.count"),
            featureMap = tileColors
            )

plotMetrics(tileMetrics[(names(tileMetrics) %in% c("(0.7,0.8]", "(0.8,0.9]", "(0.9,1]"))],
            plotList = c("cumulative","count","delta.count"),
            featureMap = tileColors
            )

plotMetrics(tileMetrics[(names(tileMetrics) %in% c("(0.4,0.5]", "(0.5,0.6]", "(0.6,0.7]"))],
            plotList = c("cumulative","count","delta.count"),
            featureMap = tileColors
            )

plotMetrics(tileMetrics[(names(tileMetrics) %in% c("[0,0.2]",   "(0.2,0.3]", "(0.3,0.4]"))],
            plotList = c("cumulative","count","delta.count"),
            featureMap = tileColors
            )

plotMetrics(tileMetrics[(names(tileMetrics) %in% c("(0.2,0.3]", "(0.3,0.4]"))],
            plotList = c("cumulative","count","delta.count"),
            featureMap = tileColors
            )


plotMetrics(tileMetrics[(names(tileMetrics) %in% c("[0,0.2]"))],
            plotList = c("cumulative","count","delta.count"),
            featureMap = tileColors,
            cumulative_points_params = list(lwd = 4, col = tileColors["[0,0.2]"]),
            delta_points_params = list(lwd = 4, col = tileColors["[0,0.2]"])
            )

# cumulative_plot_params = list(mar = c(0,6,3,1))




```

By visual inspection, here's a few things I'm noticing:
First of all, the headcount is completely unexpected.
At least the smallest bucket -- 0 to 0.2 -- has roughly double the headcount as the others.  And the steepest decline (interesting!)

However, how does the top 90th percentile have the largest headcount?  Clearly?  Maybe they have the most stable population?  I mean, that's what this graph is showing, isn't it?  So the people who are in this elite group tend to stick around.  That's positive.  (But don't forget this is 'incomplete' data with a broad assumption.)



The headcount for the other lines really start to diverge at about 2020.

As far as the decline, could this mean that new PI's simply haven't had the time to enter into the 90th percentile?  After all, these percentiles are calculated over a ten year time.

Which makes me wonder how much of this is circular reasoning.  That is, I have a very large drop-off in PI's at the bottom.   And, are they at the bottom because they left?  Are the PI's at the top because they stayed?  If a PI currently at the top left -- then wouldn't their lifetime  total drop, and their lifetime percentile drop, and they wouldn't be at the top?  

Is it that they are stable, and that puts them in the top?  Or is it that they are at the top, and that makes them stable?

A twin graphic to this is the amount won per year by percentile.
That would make a reasonably nice line graph.


```{r}

###############################
## ANNUAL AMOUNT WON BY TILE ##
###############################

popper <- merge(prepData, piClusters[[2]][,c("emplid", "win.sum_percentile", "win.sum_tile")], by.x = "PROPOSAL_PI_EMPLID", by.y = "emplid" )

# I've probably already created this graphic in my library
# I could use the "trends" one

# looks like I want 

# calculateWinRates |> extractColumn |> createFrame |> extractTrendClusters |> plotTrends

tileRates <- calculateWinRates(data = popper,
                  targetColumn = "PROPOSAL_TOTAL_SPONSOR_BUDGET",
                  categoryColumn = "win.sum_tile")

plotWinRates(tileRates[[1]], agg = "sum") 

tile_by_year <- lapply(levels(popper$win.sum_tile), function(tile){
  
  theCut <- popper[popper$win.sum_tile == tile,]
  theAgg <- calculateWinRates(data = theCut, categoryColumn = "upload_fiscal_year", functionList = list(mean = mean, median = median)) |>
    (\(x){ return(x$summary[order(as.numeric(row.names(x$summary)), decreasing = FALSE),]) })()
  
  return(theAgg)  
  
})
names(tile_by_year) <- levels(popper$win.sum_tile)


tileMap <- cbind(college = levels(popper$win.sum_tile),
                 abbrv = levels(popper$win.sum_tile),
                 color = tileColors,
                 pch = 0:8,
                 cex = NA
                 )


extractColumn(tile_by_year, "win.sum", drop = FALSE) |>
  (\(x){ 
  suppressWarnings(createFrame(x)) })() |>
#    (\(x){data.frame(x[,1], scale(x[,-1]))})() |> # scale
#  (\(x){x[x == 0] <- NA; # log
#  data.frame(x[,1], log(x[,-1]));
#  })() |>
  extractTrendClusters(k=3, type = "slope", dist_params = list(method = "maximum") ,hclust_params =list(method = "ward.D2")) |>
  plotTrends(
             detailMapping = tileMap,
             plot_order = "slope.increasing",
             bottom_axis_params = list(outer = FALSE, line = -3.5, tick = FALSE), 
             right_axis_params = list(line = -3, tick = FALSE),
             plot_params = list(oma = c(1,0,3,1), mar = c(0,7,0,0)),
             mtext_params = list(text = ""),
             legend_params = list(cex=1.5, inset = c(-0.09,0), text.font = 2),
             main_params=list(text = "Funds requested won per tile", cex = 1.5))

# oh, I get it 
# o.k.

# really close to what I want
# It's just not quite what I want
# but it's really close


extractColumn(tile_by_year, "win.sum", drop = FALSE) |>
  (\(x){ 
  suppressWarnings(createFrame(x)) })() |>
    (\(x){
      keepNames <- names(x)
      scaled <- data.frame(x[,1], scale(x[,-1]))
      names(scaled) <- keepNames
      return(scaled)
      
      })() |> # scale
#  (\(x){x[x == 0] <- NA; # log
#  data.frame(x[,1], log(x[,-1]));
#  })() |>
  extractTrendClusters(k=3, type = "slope", dist_params = list(method = "maximum") ,hclust_params =list(method = "ward.D2")) |>
  plotTrends(
             detailMapping = tileMap,
             plot_order = "slope.increasing",
             bottom_axis_params = list(outer = FALSE, line = -3.5, tick = FALSE), 
             right_axis_params = list(line = -3, tick = FALSE),
             plot_params = list(oma = c(1,0,3,1), mar = c(0,7,0,0)),
             mtext_params = list(text = ""),
             legend_params = list(cex=1.5, inset = c(-0.09,0), text.font = 2),
             main_params=list(text = "Funds requested won per tile", cex = 1.5))

# also very interesting
# but not quite what I want

# I want one graphic and one line per tile


tile_by_year_win.sum <- extractColumn(tile_by_year, "win.sum", drop = FALSE) |>
  (\(x){ 
  suppressWarnings(createFrame(x)) })()


plot(x = max(tile_by_year_win.sum[,-1]),
     type = "n",
     ylim = range(tile_by_year_win.sum[,-1]),
     xlim = c(1,nrow(tile_by_year_win.sum)),
     xaxt = "n"
     )

invisible(
lapply(colnames(tile_by_year_win.sum[,-1]), function(tile){
  points(tile_by_year_win.sum[,tile],
         type = "l",
         col = tileColors[tile]
         )
  
})
)

# wow, they are really pulling it in at the top!

# I should do a second graph that is the bottom two for scaling purpose

# I think I'm getting close to completing this as it is, including my add'l questions

# That is, something like, I'll re-name this as "Retention Exploratory Data Analysis 'Partial'" or something like that.

# So I want to graphically clean this up a bit
# Do two graphs, following my golden ratio rule
# bottom plot excludes the top two lines

# I should probably still calculate a year-by-year percentile
# and see how often a PI changes categories


```


```{r}

######################################
## PERCENTILE BY YEAR PLOT FUNCTION ##
######################################

plotAnnualPercentiles <- function(data, sequence = NA, lineColor = NA, rangeSpan = NA){
  
  # Create an empty plot
  
  plot(x= 1,
       type = "n",
       ylim = c(0.3,1),
       xlim = c(1,10),
       las = 1,
       ylab = "",
       xlab = "",
       xaxt = "n"
  )
  
  if(is.na(sequence[1])){
  theSequence <- seq(0,0.7, by = 0.05)
  } else {theSequence <- sequence}
  
  if(is.na(lineColor[1])){
  lineColor <- viridis::inferno(length(theSequence), direction = -1) 
  }
  
  if(is.na(rangeSpan)){
    rangeSpan <- 0.05 
  }
  
  # Plot the lines
  
  invisible(
    lapply(theSequence, function(range_value){ 
      
      filter <- data$range > range_value &  data$range <= range_value + rangeSpan
      
      invisible(
        lapply(data[filter,"emplid"], function(pi){
          
          points(as.numeric(data[data$emplid == pi,as.character(2014:2023)]),
                 type = "l",
                 lwd = 0.2,
                 col = lineColor[which(theSequence == range_value)]
          )
          
        })
      )
      
    })
  )
    
  # Axis and texts
  
  axis(side = 1, at = 1:10, labels = 2014:2023)
  mtext(side = 2, "percentile per PI", line = 3)
  mtext(side = 3, text = "Annual percentile per PI", font = 2, cex = 1.619, line = 2)
  
}



```

```{r}

######################## 
## PERCENTILE BY YEAR ##
########################

# This will take a little bit of logic

# first I filter by year
# then I aggregate by pi
# then I calculate percentile

pi_by_year <- lapply(unique(prepData$upload_fiscal_year)[order(unique(prepData$upload_fiscal_year))], function(fyear){
  
  theCut <- prepData[prepData$upload_fiscal_year == fyear,]
  theAgg <- calculateWinRates(data = theCut,
                              categoryColumn = "PROPOSAL_PI_EMPLID",
                              functionList = list(mean = mean, median = median)) |>
    (\(x){ return(x$summary[order(as.numeric(row.names(x$summary)), decreasing = FALSE),]) })()
  
  return(theAgg)
})
names(pi_by_year) <- unique(prepData$upload_fiscal_year)[order(unique(prepData$upload_fiscal_year))]


# Functions
# piEmplid$win.sum_prop <- proportions(piEmplid$win.sum)
# piEmplid$win.sum_rank <- rank(-piEmplid$win.sum,na.last = "keep", ties.method = "min")
# ecdf_fun <- ecdf(piEmplid$win.sum)
# piEmplid$win.sum_percentile <- ecdf_fun(piEmplid$win.sum)


pi_by_year <- lapply(pi_by_year, function(fyear){
  
  fyear[,"win.sum_prop"] <- proportions(fyear[,"win.sum"])
  fyear[,"win.sum_rank"] <- rank(-fyear[,"win.sum"], na.last = "keep", ties.method = "min")
  ecdf_fun <- ecdf(fyear[,"win.sum"])
  fyear[,"win.sum_percentile"] <- ecdf_fun(fyear[,"win.sum"])
  
  return(fyear)
  
}) 

# now I want to see the PI's that change percentiles from year to year

pi_annual_percentile <- extractColumn(pi_by_year, "win.sum_percentile", drop = FALSE) |>
  (\(x){ 
  suppressWarnings(createFrame(x)) })()
names(pi_annual_percentile)[names(pi_annual_percentile) %in% "year"] <- "emplid"

pi_annual_percentile$range <- apply(pi_annual_percentile[,-1], 1, function(x){diff(range(x, na.rm = TRUE))})        

# let's plot this

hPlot <- hist(pi_annual_percentile$range, plot = FALSE)

theBar <- barplot(hPlot$counts/sum(hPlot$counts),
        las = 2,
        names.arg = hPlot$breaks[-1])

rect(xleft = par("usr")[1],
     ybottom = par("usr")[3],
     xright = par("usr")[2],
     ytop = par("usr")[4],
     col = "grey95"
     )

grid(ny = NULL, nx = NA, lwd =3, col = "grey80")

#barplot(theBar, ann = FALSE, add = TRUE)

barplot(hPlot$counts/sum(hPlot$counts),
        las = 2,
        col = "plum3",
        # names.arg = hPlot$breaks[-1],
        plot = TRUE,
        ann = FALSE,
        add = TRUE)

mtext(side = 3, text = "Span between min and max percentile", font = 2, cex = 1.619, line = 2)

mtext(side = 3, text = "per PI over ten years", font = 3, cex = 1.33, line = 0.8)

mtext(side = 1, text = "Span between min and max percentile", line = 3)

mtext(side = 2, text = "Proportion of PI's", line = 3)

```


```{r eval = FALSE}

# Developing year by year plots

# now let's see some of these dramatic ones
# and then let's compare to the ten yr categories

plot(x= 1,
     type = "n",
     ylim = c(0,1),
     xlim = c(1,10))

filter <- pi_annual_percentile$range > 0.6 &  pi_annual_percentile$range <= 0.7
invisible(
lapply(pi_annual_percentile[filter,"emplid"], function(pi){
  
  points(as.numeric(pi_annual_percentile[pi_annual_percentile$emplid == pi,as.character(2014:2023)]),
         type = "l",
         lwd = 0.2,
         col = "red")
  
})
)

# since this is exploratory data analysis, let's show it.
  
plot(x= 1,
     type = "n",
     ylim = c(0.3,1),
     xlim = c(1,10),
     las = 1,
     ylab = "",
     xlab = "",
     xaxt = "n"
     )

theSequence <- seq(0,0.7, by = 0.05)

invisible(
lapply(theSequence, function(range_value){ 

filter <- pi_annual_percentile$range > range_value &  pi_annual_percentile$range <= range_value + 0.05

invisible(
lapply(pi_annual_percentile[filter,"emplid"], function(pi){
  
  points(as.numeric(pi_annual_percentile[pi_annual_percentile$emplid == pi,as.character(2014:2023)]),
         type = "l",
         lwd = 0.2,
         col = viridis::inferno(length(theSequence), direction = -1)[which(theSequence == range_value)]
         )
  
})
)

})
)

axis(side = 1, at = 1:10, labels = 2014:2023)

mtext(side = 2, "percentile per PI", line = 3)
mtext(side = 3, text = "Annual percentile per PI", font = 2, cex = 1.619, line = 2)


# Let's show low variation, mid-variation, and hi-variation
# which means I'm creating a function.


```

```{r}

theSequence <- seq(0, 0.7, 0.05)

plotAnnualPercentiles(data = pi_annual_percentile)
mtext(side = 3, text = "All PI's", line = 1)


plotAnnualPercentiles(data = pi_annual_percentile, sequence = c(0.0,0.05),
                      lineColor = c("plum4"))
mtext(side = 3, text = "Low variation", line = 1)

plotAnnualPercentiles(data = pi_annual_percentile, sequence = c(0.25,0.35), lineColor = viridis::inferno(length(theSequence), direction = -1)[vapply(seq(from = 0.25, to = 0.35, by = 0.05), function(val) which.min(abs(seq(0, 0.7, 0.05) - val)), integer(1))]
                 )
mtext(side = 3, text = "Medium variation", line = 1)

plotAnnualPercentiles(data = pi_annual_percentile, sequence = c(0.4,0.6),
                      lineColor = viridis::inferno(length(theSequence), direction = -1)[vapply(seq(from = 0.4, to = 0.6, by = 0.05), function(val) which.min(abs(seq(0, 0.7, 0.05) - val)), integer(1))])
mtext(side = 3, text = "High variation", line = 1)

# Now, it would be nice to see a plot of these per cluster
# And, to see not just the percentile but whatever y-axis I want
# More work on this function would be nice


```


```{r eval=FALSE}

# Copying and pasting from other places

# FISCAL YEAR -- BY COLLEGE (loaded in prepScript)

college_by_year <- lapply(unique(prepData$college), function(college){
  
  theCut <- prepData[prepData$college == college,]
  theAgg <- calculateWinRates(data = theCut, categoryColumn = "upload_fiscal_year", functionList = list(mean = mean, median = median)) |>
    (\(x){ return(x$summary[order(as.numeric(row.names(x$summary)), decreasing = FALSE),]) })()
  
  return(theAgg)  
  
})
names(college_by_year) <- unique(prepData$college)

extractColumn(college_by_year, "win.sum", drop = FALSE) |>
  createFrame() |>
    (\(x){data.frame(x[,1], scale(x[,-1]))})() |> # scale
#  (\(x){x[x == 0] <- NA; # log
#  data.frame(x[,1], log(x[,-1]));
#  })() |>
  extractTrendClusters(k=3, type = "slope", dist_params = list(method = "maximum") ,hclust_params =list(method = "ward.D2")) |>
  plotTrends(detailMapping = collegeAbbrv,
             plot_order = "slope.increasing",
             bottom_axis_params = list(outer = FALSE, line = -3.5, tick = FALSE), 
             right_axis_params = list(line = -3, tick = FALSE),
             plot_params = list(oma = c(1,0,3,1), mar = c(0,7,0,0)),
             mtext_params = list(text = ""),
             legend_params = list(cex=1.5, inset = c(-0.09,0), text.font = 2),
             main_params=list(text = "Funds requested won per college", cex = 1.5))

```



This portion is now fixed:
I am not understanding in the slightest how I have the largest headcount in the 90th percentile and then going down from there.

This needs some looking into.

Fixed -- by using piCluster[[2]] instead of piEmplid, where piCluster[[2]] uses the full population.

so now I am understanding it --- I calculated the percentile based on the full population, but then filtered to just PI's with at least three or four proposals, so my population is heavier at the higher percentiles.
using piCluster[[2]] should correct that.




# DATA DESCRIPTION

```{r include=TRUE}

skim(retData)

```

# QUERY

VPR.D_PI_EMP_DT_VW as
  SELECT pi."PI_DIM_KEY",    
         pi."PI_EMPLID",    
         pi."PI_FIRST_NAME",    
         pi."PI_MIDDLE_NAME",    
         pi."PI_LAST_NAME",    
         pi."PI_NAME",    
         pi."PI_EMAIL_ADDRESS",    
         pi."PI_PHONE",    
         pi."PI_INDICATOR",    
         pi."IS_PI",    
         pi."PI_LOAD_DATE_TIME",    
         pi."PI_UPDATE_DATE_TIME",    
         emp.hire_dt,    
         emp.rehire_dt,    
         emp.termination_dt
         FROM osp.d_pi_vw pi
         LEFT JOIN uuetl_hr.PS_UU_EMPLOYMENT_VW emp ON pi.pi_emplid = emp.emplid
 
#3) COMBINE RETENTION AND PROPOSAL DATA

  3.  Describe the combination of the retention data and the proposal data.
 

```{r}

###############
## DURATIONS ##
###############


histPar <- par(mfrow = c(3,1),
               mar = c(2,3,3,0),
               bg = "ivory",
               fg = "gray10")

minTwoCondition <- activeProposing$PROPOSAL_PI_EMPLID %in% row.names(fullEmplid)[filterTwoCount]
time_length(interval(activeProposing$PROPOSAL_UPLOAD_DATE.min[minTwoCondition], activeProposing$PROPOSAL_UPLOAD_DATE.max[minTwoCondition]), unit = "year") |>
  hist(main = "Active proposing duration\n(from first to last proposal date in years)\n(minimum of two proposals)",
               col = "skyblue",
       ylab = "count of PIs")

time_length(interval(activeProposing$PROPOSAL_UPLOAD_DATE.max, activeProposing$TERMINATION_DT), unit = "year") |>
  hist(main = "Inactive period from last proposal to termination (years)",
               col = "deepskyblue",
       ylab = "count of PIs")

condition <- is.na(activeProposing$TERMINATION_DT)
 time_length(interval(activeProposing$PROPOSAL_UPLOAD_DATE.max[condition], ymd("2025-05-01")), unit = "year") |>
   hist(main = "Inactive period from last proposal to 30 April 2025 \n(employed as of data cut-off of 30 April 2025)",
               col = "dodgerblue",
       ylab = "count of PIs")

# Maybe active needs to be filtered to PI's with at least two proposals? DONE
 
 
# For people active in the hire date -- how long from hire until submission?
# For people active in the rehire date -- how long from re-hire until submission?
# For all of them -- how long from last submission until termination?

# Hire until first submission

# I can use filters here or I can create an "effective hire date" 
 
time_length(interval(activeProposing$effective_hire, activeProposing$PROPOSAL_UPLOAD_DATE.min), unit = "year") |> # max() # 50 yrs!
  hist(main = "Period from effective hire to first proposal")

# something is still wrong

time_length(interval(activeProposing$effective_hire, activeProposing$PROPOSAL_UPLOAD_DATE.min), unit = "year") |>
  (\(x){which(x == max(x))})()

# the problem is that I don't have good proposal data until 2013.
# so I should truncate this to people hired after 2013

# make this a precise date, the minimum in the prepData
hire2013 <- year(activeProposing$effective_hire) >= 2013
time_length(interval(activeProposing$effective_hire[hire2013], activeProposing$PROPOSAL_UPLOAD_DATE.min[hire2013]), unit = "year") |> # max() # 50 yrs!
  hist(main = "Period from effective hire to first proposal\n(for hires after 2013)")

# there we go, that's a much better graphic


```

Before I compare turnover by cluster, I need to add this descriptive bit:

all(prepData$PROPOSAL_PI_EMPLID %in% retData$PI_EMPLID) # TRUE (good)

table(retData$PI_EMPLID %in% prepData$PROPOSAL_PI_EMPLID)
FALSE  TRUE 
1538  2937 

And discuss how there are more PI's in the retData than are in my proposal data set, and that talking about clusters introduces a filter.

Presumably these 1,538 "FALSE" ones haven't proposed after 2013.    

It's probably a good time to disect that query a little more.
.... and here's what I'm seeing:

a "left join" onto osp.d_pi_vw.

So now I want to know how osp.d_pi_vw was created (how is it defined?) and how did it get 1,538 PI's that aren't in my proposal data?

Let's check that out --

My "prepData" had some cleaning on it.


...and how worth is my time to chase this down when I'm likely to get new information and better data after talking to HR?  I think I'll leave this one down.

It does raise the question.  From my complexMetrics, what's the first hire date?




         
         